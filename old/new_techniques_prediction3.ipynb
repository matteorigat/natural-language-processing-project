{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:10:27.037132Z",
     "start_time": "2025-04-26T11:10:27.023389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# # 1. Imports and Initial Setup\n",
    "#\n",
    "# Import necessary libraries and download NLTK data.\n",
    "\n",
    "# %%\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast  # To safely evaluate string literals\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# NLP Preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# NLTK Data Download (run once if needed)\n",
    "def download_nltk_resource(resource_id, resource_name):\n",
    "    try:\n",
    "        nltk.data.find(resource_name)\n",
    "    except LookupError:\n",
    "        print(f\"Downloading NLTK resource: {resource_id}\")\n",
    "        nltk.download(resource_id)\n",
    "\n",
    "\n",
    "download_nltk_resource('stopwords', 'corpora/stopwords')\n",
    "download_nltk_resource('punkt', 'tokenizers/punkt')\n",
    "\n",
    "# Initialize global resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Compile regex for efficiency\n",
    "non_alpha_regex = re.compile('[^a-zA-Z ]')\n",
    "\n",
    "# Configure tqdm for pandas\n",
    "tqdm.pandas()"
   ],
   "id": "fe1fd34f7519c0f5",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:10:27.049396Z",
     "start_time": "2025-04-26T11:10:27.043439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# # 2. Preprocessing Functions (Phrase-Aware for Word2Vec)\n",
    "#\n",
    "# Define preprocessing functions. The key change is treating multi-word ingredients and techniques as single tokens for Word2Vec by joining them with underscores.\n",
    "\n",
    "# %%\n",
    "def preprocess_item_list_for_w2v(item_list: list) -> list:\n",
    "    \"\"\"\n",
    "    Preprocesses a list of ingredient or technique strings for Word2Vec.\n",
    "    - Lowercase\n",
    "    - Removes non-alphabetic characters (keeping spaces initially)\n",
    "    - Replaces spaces within items with underscores to treat them as single tokens.\n",
    "    - Filters out very short items and stopwords (optional, here we keep stopwords\n",
    "      to preserve phrases like 'cream_of_tartar').\n",
    "    Returns a list of processed, unique strings (tokens/phrases).\n",
    "    \"\"\"\n",
    "    processed_items = set()\n",
    "    for item_phrase in item_list:\n",
    "        # Basic cleaning: lowercase, remove non-alpha (keep spaces for now)\n",
    "        cleaned_phrase = non_alpha_regex.sub('', item_phrase.lower()).strip()\n",
    "        # Remove extra spaces\n",
    "        cleaned_phrase = re.sub(r'\\s+', ' ', cleaned_phrase).strip()\n",
    "\n",
    "        if cleaned_phrase and len(cleaned_phrase) > 1:\n",
    "            # Replace spaces with underscores to form single tokens\n",
    "            token_phrase = cleaned_phrase.replace(' ', '_')\n",
    "            # Optional: Add further filtering if needed (e.g., length)\n",
    "            # if len(token_phrase) > 2: # Example filter\n",
    "            processed_items.add(token_phrase)\n",
    "\n",
    "    return list(processed_items)\n",
    "\n",
    "\n",
    "# --- Helper Function for String List Evaluation ---\n",
    "def safe_eval_list_string(list_string: str) -> list:\n",
    "    \"\"\"Safely evaluate a string representation of a list.\"\"\"\n",
    "    if not isinstance(list_string, str): return []\n",
    "    try:\n",
    "        evaluated = ast.literal_eval(list_string)\n",
    "        return evaluated if isinstance(evaluated, list) else []\n",
    "    except (ValueError, SyntaxError, TypeError):\n",
    "        return []"
   ],
   "id": "effb7e41cb44871",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:10:35.803762Z",
     "start_time": "2025-04-26T11:10:27.066452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# # 3. Data Loading and Preprocessing Execution\n",
    "#\n",
    "# Load the dataset and apply the new phrase-aware preprocessing.\n",
    "\n",
    "# %%\n",
    "DATASET_PATH = '../dataset/RAW_merged.csv'  # Make sure this path is correct\n",
    "df = pd.DataFrame()  # Initialize df\n",
    "\n",
    "try:\n",
    "    print(f\"Loading dataset from: {DATASET_PATH}\")\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    print(f\"Dataset loaded with {len(df)} rows.\")\n",
    "\n",
    "    # Drop rows with missing essential data\n",
    "    df.dropna(subset=['ingredients', 'techniques_list'], inplace=True)\n",
    "    print(f\"Rows after removing NaNs in ingredients/techniques: {len(df)}\")\n",
    "\n",
    "    print(\"Converting string columns to lists...\")\n",
    "    # Use progress_apply for visual feedback with tqdm\n",
    "    df['ingredients_list'] = df['ingredients'].progress_apply(safe_eval_list_string)\n",
    "    df['techniques_list_eval'] = df['techniques_list'].progress_apply(safe_eval_list_string)\n",
    "\n",
    "    print(\"Applying phrase-aware preprocessing for Word2Vec...\")\n",
    "    # Apply the NEW preprocessing to both ingredients and techniques\n",
    "    df['ingredients_processed_w2v'] = df['ingredients_list'].progress_apply(preprocess_item_list_for_w2v)\n",
    "    df['techniques_processed_w2v'] = df['techniques_list_eval'].progress_apply(preprocess_item_list_for_w2v)\n",
    "\n",
    "    # Display processed data preview\n",
    "    print(\"\\nPreview of the processed DataFrame:\")\n",
    "    print(df[['ingredients_list', 'ingredients_processed_w2v', 'techniques_list_eval',\n",
    "              'techniques_processed_w2v']].head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset file not found at '{DATASET_PATH}'\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during data loading/preprocessing: {e}\")\n"
   ],
   "id": "c935f0043c1caf78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: dataset/RAW_merged.csv\n",
      "Dataset loaded with 178265 rows.\n",
      "Rows after removing NaNs in ingredients/techniques: 178265\n",
      "Converting string columns to lists...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/178265 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5805caed182145b1969e40edd61de5e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/178265 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "650aa5758b2a49488ca14cedd567584b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying phrase-aware preprocessing for Word2Vec...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/178265 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efdadfd067174b83b40bf1ab816a913b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/178265 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98909fbc448e4514a29129b33b4a11de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview of the processed DataFrame:\n",
      "                                    ingredients_list  \\\n",
      "0  [winter squash, mexican seasoning, mixed spice...   \n",
      "1  [prepared pizza crust, sausage patty, eggs, mi...   \n",
      "2  [spreadable cheese with garlic and herbs, new ...   \n",
      "3  [milk, vanilla ice cream, frozen apple juice c...   \n",
      "4  [fennel seeds, green olives, ripe olives, garl...   \n",
      "\n",
      "                           ingredients_processed_w2v  \\\n",
      "0  [olive_oil, winter_squash, honey, mexican_seas...   \n",
      "1  [eggs, milk, sausage_patty, prepared_pizza_cru...   \n",
      "2  [shallots, parsley, olive_oil, new_potatoes, r...   \n",
      "3  [apple, vanilla_ice_cream, frozen_apple_juice_...   \n",
      "4  [garlic, orange_juice, orange_rind, green_oliv...   \n",
      "\n",
      "                    techniques_list_eval  \\\n",
      "0                    [bake, grate, melt]   \n",
      "1                    [bake, pour, whisk]   \n",
      "2              [bake, boil, dice, drain]   \n",
      "3               [blend, combine, smooth]   \n",
      "4  [crush, marinate, refrigerate, toast]   \n",
      "\n",
      "                techniques_processed_w2v  \n",
      "0                    [melt, bake, grate]  \n",
      "1                    [pour, bake, whisk]  \n",
      "2              [dice, boil, bake, drain]  \n",
      "3               [combine, blend, smooth]  \n",
      "4  [crush, refrigerate, toast, marinate]  \n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:10:36.624354Z",
     "start_time": "2025-04-26T11:10:35.909675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# # 4. Word2Vec Model Training\n",
    "#\n",
    "# Train the Word2Vec model using the processed ingredients and techniques, where multi-word items are treated as single tokens (e.g., 'tomato_puree').\n",
    "\n",
    "# %%\n",
    "model_w2v = None  # Initialize model variable\n",
    "\n",
    "if not df.empty and 'ingredients_processed_w2v' in df.columns and 'techniques_processed_w2v' in df.columns:\n",
    "    # Word2Vec Parameters\n",
    "    VECTOR_SIZE = 100  # Dimensionality of the word vectors\n",
    "    WINDOW_SIZE = 5  # Context window size\n",
    "    MIN_COUNT = 3  # Ignore words/phrases with frequency lower than this\n",
    "    SG_MODEL = 1  # 1 for Skip-gram, 0 for CBOW\n",
    "    EPOCHS = 5  # Number of training iterations (increase for potentially better results)\n",
    "    WORKERS = -1  # Use all available CPU cores\n",
    "\n",
    "    print(\"Preparing data for Word2Vec training...\")\n",
    "    # Combine ingredient and technique lists for each recipe\n",
    "    # Each item in the list is now a potential phrase (e.g., 'tomato_puree' or 'bake')\n",
    "    corpus_for_w2v = df['ingredients_processed_w2v'] + df['techniques_processed_w2v']\n",
    "    # Filter out any potentially empty lists that resulted from preprocessing\n",
    "    corpus_for_w2v = [item_list for item_list in corpus_for_w2v if item_list]\n",
    "    if corpus_for_w2v:\n",
    "        print(f\"Training Word2Vec model on {len(corpus_for_w2v)} documents...\")\n",
    "        # Train the model - corpus_for_w2v is already a list of lists of \"tokens\" (words or underscore_phrases)\n",
    "        model_w2v = Word2Vec(sentences=corpus_for_w2v,\n",
    "                             vector_size=VECTOR_SIZE,\n",
    "                             window=WINDOW_SIZE,\n",
    "                             min_count=MIN_COUNT,\n",
    "                             sg=SG_MODEL,\n",
    "                             epochs=EPOCHS,\n",
    "                             workers=WORKERS)\n",
    "        print(\"Word2Vec training completed.\")\n",
    "        print(f\"Vocabulary size: {len(model_w2v.wv.index_to_key)}\")\n",
    "\n",
    "        # Optional: Save the model for later use\n",
    "        # model_w2v.save(\"cooking_word2vec.model\")\n",
    "        # print(\"Model saved to cooking_word2vec.model\")\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: No valid textual data found to train Word2Vec after preprocessing.\")\n",
    "else:\n",
    "    print(\"ERROR: DataFrame is empty or required processed columns are missing. Cannot train Word2Vec.\")\n"
   ],
   "id": "3fd307ec4f5fba9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for Word2Vec training...\n",
      "Training Word2Vec model on 178265 documents...\n",
      "Word2Vec training completed.\n",
      "Vocabulary size: 8716\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:10:36.702307Z",
     "start_time": "2025-04-26T11:10:36.633166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# # 5. Prepare Technique Vectors for Prediction\n",
    "#\n",
    "# Extract the vectors for all unique processed techniques from the trained Word2Vec model.\n",
    "\n",
    "# %%\n",
    "techniques_vectors = {}  # Dictionary to store {technique_phrase: vector}\n",
    "\n",
    "if model_w2v:\n",
    "    print(\"Extracting technique vectors from the trained Word2Vec model...\")\n",
    "    # Get unique techniques from the processed column\n",
    "    if 'techniques_processed_w2v' in df.columns:\n",
    "        # Explode the list column, drop NaNs, get unique values\n",
    "        unique_processed_techniques = df['techniques_processed_w2v'].explode().dropna().unique()\n",
    "        print(f\"Found {len(unique_processed_techniques)} unique processed techniques.\")\n",
    "\n",
    "        count_found = 0\n",
    "        count_missing = 0\n",
    "        missing_techniques = []\n",
    "\n",
    "        for technique in unique_processed_techniques:\n",
    "            if technique in model_w2v.wv:\n",
    "                techniques_vectors[technique] = model_w2v.wv[technique]\n",
    "                count_found += 1\n",
    "            else:\n",
    "                # This happens if a technique appeared less than MIN_COUNT times\n",
    "                count_missing += 1\n",
    "                missing_techniques.append(technique)\n",
    "\n",
    "        print(f\"Successfully extracted vectors for {count_found} techniques.\")\n",
    "        if count_missing > 0:\n",
    "            print(\n",
    "                f\"WARNING: {count_missing} techniques were not found in the Word2Vec vocabulary (likely due to min_count={MIN_COUNT}).\")\n",
    "            # print(f\"Missing techniques sample: {missing_techniques[:20]}\") # Optional: show missing ones\n",
    "    else:\n",
    "        print(\"ERROR: Column 'techniques_processed_w2v' not found in DataFrame.\")\n",
    "else:\n",
    "    print(\"ERROR: Word2Vec model ('model_w2v') is not available. Cannot extract technique vectors.\")\n",
    "\n",
    "# Optional: Inspect some extracted vectors\n",
    "if techniques_vectors:\n",
    "    print(\"\\nSample of techniques with extracted vectors:\")\n",
    "    print(list(techniques_vectors.keys())[:15])\n"
   ],
   "id": "2ad5c4535035494f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting technique vectors from the trained Word2Vec model...\n",
      "Found 57 unique processed techniques.\n",
      "Successfully extracted vectors for 57 techniques.\n",
      "\n",
      "Sample of techniques with extracted vectors:\n",
      "['melt', 'bake', 'grate', 'pour', 'whisk', 'dice', 'boil', 'drain', 'combine', 'blend', 'smooth', 'crush', 'refrigerate', 'toast', 'marinate']\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:10:36.717208Z",
     "start_time": "2025-04-26T11:10:36.711310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# # 6. Prediction Function (Average Vector Similarity)\n",
    "#\n",
    "# Define the prediction function using the average vector approach. It calculates a single vector for the input ingredients and finds the closest technique vectors.\n",
    "\n",
    "# %%\n",
    "def predict_cooking_methods_avg_vector(ingredients_str: str,\n",
    "                                       technique_vectors_dict: dict,\n",
    "                                       w2v_model: Word2Vec,\n",
    "                                       top_n: int = 5,\n",
    "                                       debug: bool = False) -> list or str:\n",
    "    \"\"\"\n",
    "    Predicts cooking techniques based on similarity to the average vector of input ingredients.\n",
    "    \"\"\"\n",
    "    if not w2v_model: return \"Error: Word2Vec model not available.\"\n",
    "    if not technique_vectors_dict: return \"Error: No technique vectors available.\"\n",
    "    if not ingredients_str: return \"Error: Input ingredients string is empty.\"\n",
    "\n",
    "    # --- 1. Preprocess input ingredients (using the same method as training) ---\n",
    "    ingredient_input_list = [ing.strip() for ing in ingredients_str.split(',') if ing.strip()]\n",
    "    if not ingredient_input_list: return \"Input ingredients list is empty after cleaning.\"\n",
    "    processed_input_ingredients = preprocess_item_list_for_w2v(ingredient_input_list)\n",
    "\n",
    "    if debug: print(f\"DEBUG: Processed input ingredients (for W2V lookup): {processed_input_ingredients}\")\n",
    "\n",
    "    # --- 2. Get vectors for valid input ingredients and calculate average vector ---\n",
    "    valid_ingredient_vectors = []\n",
    "    not_found_ingredients = []\n",
    "    for ingredient_token in processed_input_ingredients:\n",
    "        if ingredient_token in w2v_model.wv:\n",
    "            valid_ingredient_vectors.append(w2v_model.wv[ingredient_token])\n",
    "        else:\n",
    "            not_found_ingredients.append(ingredient_token)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"DEBUG: Found vectors for {len(valid_ingredient_vectors)} ingredients.\")\n",
    "        if not_found_ingredients: print(f\"DEBUG: Ingredients not found in W2V vocab: {not_found_ingredients}\")\n",
    "\n",
    "    if not valid_ingredient_vectors:\n",
    "        return \"None of the input ingredients were found in the model's vocabulary.\"\n",
    "\n",
    "    # Calculate the average vector for the input ingredients\n",
    "    average_ingredient_vector = np.mean(valid_ingredient_vectors, axis=0).reshape(1, -1)\n",
    "\n",
    "    if debug: print(f\"DEBUG: Calculated average ingredient vector shape: {average_ingredient_vector.shape}\")\n",
    "\n",
    "    # --- 3. Calculate similarity between average ingredient vector and all technique vectors ---\n",
    "    technique_similarities = {}\n",
    "    tech_items = list(technique_vectors_dict.items())  # List of (technique_name, vector)\n",
    "\n",
    "    for technique, tech_vector in tech_items:\n",
    "        try:\n",
    "            # Ensure tech_vector is valid numpy array for cosine_similarity\n",
    "            if isinstance(tech_vector, np.ndarray) and tech_vector.shape[0] == w2v_model.vector_size:\n",
    "                similarity = cosine_similarity(average_ingredient_vector, tech_vector.reshape(1, -1))[0][0]\n",
    "                if not np.isnan(similarity):  # Check for NaN results\n",
    "                    technique_similarities[technique] = similarity\n",
    "            # else: # Optional: Log invalid technique vectors\n",
    "            #    if debug: print(f\"DEBUG: Skipping invalid vector for technique: {technique}\")\n",
    "        except Exception as e:\n",
    "            if debug: print(f\"DEBUG: Error calculating similarity for {technique}: {e}\")\n",
    "\n",
    "    if not technique_similarities:\n",
    "        return \"Could not calculate similarities for any techniques.\"\n",
    "\n",
    "    # --- 4. Sort techniques by similarity and return top N ---\n",
    "    # Sort the dictionary by similarity scores in descending order\n",
    "    sorted_techniques = sorted(technique_similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    if debug: print(f\"DEBUG: Top 10 techniques by similarity: {sorted_techniques[:10]}\")\n",
    "\n",
    "    # Extract just the names of the top N techniques\n",
    "    top_predicted_techniques = [tech_name for tech_name, score in sorted_techniques[:top_n]]\n",
    "\n",
    "    return top_predicted_techniques\n"
   ],
   "id": "4d39ed539dea6a09",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:10:36.737848Z",
     "start_time": "2025-04-26T11:10:36.725384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# # 7. Example Usage of the Prediction Function\n",
    "#\n",
    "# Test the prediction function with a sample list of ingredients.\n",
    "\n",
    "# %%\n",
    "if model_w2v and techniques_vectors:\n",
    "    # Example input\n",
    "    new_ingredients_example = \"tomato puree, lemon juice, salt, oregano, basil, thyme, garlic powder\"\n",
    "    print(f\"\\nPredicting techniques for ingredients: '{new_ingredients_example}'\")\n",
    "    print(\"(Using average ingredient vector similarity approach)\")\n",
    "\n",
    "    # Call the prediction function\n",
    "    predicted_methods = predict_cooking_methods_avg_vector(\n",
    "        ingredients_str=new_ingredients_example,\n",
    "        technique_vectors_dict=techniques_vectors,\n",
    "        w2v_model=model_w2v,\n",
    "        top_n=5,\n",
    "        debug=True  # Enable debug messages\n",
    "    )\n",
    "\n",
    "    print(\"\\n-------------------------------------\")\n",
    "    if isinstance(predicted_methods, list):\n",
    "        print(f\"Predicted cooking techniques (Top {len(predicted_methods)}):\")\n",
    "        # Replace underscores with spaces for better readability in the final output\n",
    "        for i, method in enumerate(predicted_methods):\n",
    "            print(f\"{i + 1}. {method.replace('_', ' ')}\")\n",
    "    else:\n",
    "        # Print error message if prediction failed\n",
    "        print(f\"Prediction result: {predicted_methods}\")\n",
    "    print(\"-------------------------------------\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nCannot run prediction example: Word2Vec model or technique vectors are not available.\")\n",
    "    print(f\"Is model_w2v trained? {'Yes' if model_w2v else 'No'}\")\n",
    "    print(f\"Are techniques_vectors populated? {'Yes' if techniques_vectors else 'No'}\")"
   ],
   "id": "1c352358e2b04637",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting techniques for ingredients: 'tomato puree, lemon juice, salt, oregano, basil, thyme, garlic powder'\n",
      "(Using average ingredient vector similarity approach)\n",
      "DEBUG: Processed input ingredients (for W2V lookup): ['basil', 'oregano', 'lemon_juice', 'garlic_powder', 'thyme', 'salt', 'tomato_puree']\n",
      "DEBUG: Found vectors for 7 ingredients.\n",
      "DEBUG: Calculated average ingredient vector shape: (1, 100)\n",
      "DEBUG: Top 10 techniques by similarity: [('poach', 0.21975678), ('griddle', 0.17659804), ('microwave', 0.1519689), ('blend', 0.14328659), ('melt', 0.13427587), ('smooth', 0.13234997), ('brine', 0.13034436), ('drain', 0.13020164), ('combine', 0.12415667), ('shred', 0.108764276)]\n",
      "\n",
      "-------------------------------------\n",
      "Predicted cooking techniques (Top 5):\n",
      "1. poach\n",
      "2. griddle\n",
      "3. microwave\n",
      "4. blend\n",
      "5. melt\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "execution_count": 92
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
