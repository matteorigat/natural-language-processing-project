{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-26T12:52:19.995166Z",
     "start_time": "2024-06-26T12:52:16.074021Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matteorigat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:52:20.005871Z",
     "start_time": "2024-06-26T12:52:19.996808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load and also preprocess the raw data\n",
    "def load_preprocess_raw_data(raw_data):\n",
    "    '''\n",
    "    take raw recipe data and preprocess it, \n",
    "    return a list of recipe instances with special tokens\n",
    "\n",
    "    parameter: raw data\n",
    "\n",
    "    return: recipe instance list\n",
    "\n",
    "    '''\n",
    "    with open(raw_data, 'r') as f:\n",
    "        raw_dict = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    raw_list = []\n",
    "    for recipe in raw_dict.values():\n",
    "        # try/except will filter out recipes that don't have title, ingredients or instructions\n",
    "        try:\n",
    "            title = recipe['title'].replace(\"ADVERTISEMENT\", \"\")\n",
    "            ingredient_list = recipe['ingredients']\n",
    "            ingredients = \"\"\n",
    "            for ingredient in ingredient_list:\n",
    "                ingredient = ingredient.replace(\"ADVERTISEMENT\", \"\")\n",
    "                if ingredient != \"\":\n",
    "                    ingredients += ingredient + \", \"\n",
    "            instructions = recipe['instructions'].replace(\"ADVERTISEMENT\", \"\")\n",
    "            recipe_instance = '<|startofrecipe|>'+title+'<|startofingre|>'+ingredients+'<|startofinstruc|>'+instructions+'<|endofrecipe|>'\n",
    "            if len(recipe_instance) <= 2000:\n",
    "                raw_list.append(recipe_instance)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    return raw_list"
   ],
   "id": "b013163b68b372",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:52:21.623702Z",
     "start_time": "2024-06-26T12:52:20.009436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# create text list for dataset\n",
    "recipe_one_list = load_preprocess_raw_data(\"dataset2/recipes_raw_nosource_ar.json\")\n",
    "recipe_two_list = load_preprocess_raw_data(\"dataset2/recipes_raw_nosource_epi.json\")\n",
    "recipe_three_list = load_preprocess_raw_data(\"dataset2/recipes_raw_nosource_fn.json\")\n",
    "recipe_list = recipe_one_list + recipe_two_list + recipe_three_list\n",
    "\n",
    "reduced_recipe_list = random.sample(recipe_list, int(0.001 * len(recipe_list)))\n",
    "print(reduced_recipe_list[:5])\n",
    "\n",
    "train_list, test_list = np.split(reduced_recipe_list, [int(.8*len(reduced_recipe_list))])\n",
    "print('Number of train data: ', len(train_list))\n",
    "print('Number of test data: ', len(test_list))"
   ],
   "id": "37ab75ee5d862da2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|startofrecipe|>Marinated Fajita Chicken<|startofingre|>1 cup lime juice , 4 1/2 teaspoons olive oil , 2 cloves garlic, crushed , 1/2 teaspoon ground cumin , 1/2 teaspoon chili powder , 1/4 teaspoon salt , 1/4 teaspoon red pepper flakes , 5 skinless, boneless chicken breast halves , <|startofinstruc|>Whisk together the lime juice, olive oil, garlic, ground cumin, chili powder, salt, and red pepper flakes in a bowl; pour into a large resealable plastic bag.\\nPut the chicken breasts into the bag, coat with the marinade, squeeze out excess air, and seal the bag.\\nMarinate in the refrigerator for 8 hours to overnight.\\nPreheat the oven to 375 degrees F (190 degrees C).\\nRemove the chicken from the marinade and shake off excess. Discard remaining marinade. Arrange chicken breasts in a baking dish.\\nBake chicken breasts in preheated oven until no longer pink in the center and the juices run clear, about 35 minutes. An instant-read thermometer inserted into the center should read at least 165 degrees F (74 degrees C).\\nShred chicken breasts with two forks to desired texture.\\n<|endofrecipe|>', '<|startofrecipe|>Gluten-Free Crepes or Pancakes<|startofingre|>2 cups almond milk , 2 tablespoons cider vinegar , 1/2 lemon, juiced , 3/4 cup rice flour , 1/3 cup potato starch , 1/4 cup arrowroot flour , 3 tablespoons tapioca flour , 2 tablespoons coconut sugar , 1 1/2 teaspoons baking powder , 1/2 teaspoon baking soda , 1/2 teaspoon xanthan gum , 1/2 teaspoon salt , 3 tablespoons coconut oil, melted , 2 eggs , <|startofinstruc|>Whisk almond milk, vinegar, and lemon juice together in a bowl; set aside for 10 minutes.\\nMix rice flour, potato starch, arrowroot flour, tapioca flour, coconut sugar, baking powder, baking soda, xanthan gum, and salt together in a bowl. Stir coconut oil into flour mixture using a fork until evenly combined. Stir eggs into flour mixture.\\nWhisk milk mixture into flour mixture until batter is thoroughly mixed.\\nHeat a lightly oiled skillet or large pan over medium heat. Pour 1/4 cup batter into the skillet and immediately rotate the skillet until the batter evenly coats the bottom in a thin layer. Cook until the top of the crepe is no longer wet and the bottom has turned light brown, 2 to 3 minutes. Run a spatula around the edge of the skillet to loosen; flip crepe and cook until the other side has turned light brown, 2 to 3 minutes. Repeat with remaining batter.\\n<|endofrecipe|>', '<|startofrecipe|>Best of Both Worlds Roast Chicken<|startofingre|>1 (4 pound) whole chicken , 1 tablespoon butter , salt and ground black pepper to taste , 1/2 onion, coarsely chopped , 1 stalk celery, cut into 4 pieces , 1/2 lemon, cut into 4 pieces , 3 cloves garlic, sliced , 2 tablespoons butter , 1 teaspoon dried parsley , 1 teaspoon dried Italian herb seasoning , <|startofinstruc|>Preheat oven to 450 degrees F (230 degrees C).\\nRemove giblet packet from cavity of chicken and pat bird thoroughly dry with paper towels. Place 1 tablespoon butter into chicken cavity, sprinkle salt and black pepper into cavity, and stuff cavity with onion, celery, and lemon pieces. Loosen skin over breast and thigh of chicken with your fingers and insert garlic slices under the skin. Place the chicken breast side up on a rack set into a roasting pan.\\nMelt 2 tablespoons butter in a small saucepan over medium-low heat; add dried parsley and Italian seasoning. Pour half the butter-herb mixture over the chicken and rub the seasoned butter onto all parts of chicken. Reserve remaining butter mixture.\\nRoast chicken in the preheated oven for 20 minutes, then remove and turn chicken so breast side is down. Pour remaining half of seasoned butter over the chicken, brushing it over all parts of the bird. Return to oven and roast an additional 10 minutes.\\nReduce oven heat to 325 degrees F (165 degrees C). Roast chicken until skin is crisp and brown and the juices run clear, 1 hour and 20 more minutes. An instant-read meat thermometer inserted into the thickest part of a thigh, not touching bone, should read 160 degrees F (70 degrees C). Let chicken rest, uncovered, for 10 minutes before serving.\\n<|endofrecipe|>', '<|startofrecipe|>Italian Chicken Skillet<|startofingre|>1 tablespoon olive oil , 4 skinless, boneless chicken breast halves, cubed , 2 cloves garlic, chopped, or to taste , 1/2 cup red cooking wine , 1 (28 ounce) can Italian-style diced tomatoes , 8 ounces small seashell pasta , 5 ounces fresh spinach, chopped , 1 cup shredded mozzarella cheese , <|startofinstruc|>Heat the olive oil in a large skillet with a lid over medium heat, and cook and stir the chicken and garlic until the chicken is no longer pink in the center, 5 to 8 minutes. Pour the wine and diced tomatoes with their juice into the skillet, and bring to a boil over high heat while scraping any browned bits of food off of the bottom of the pan with a wooden spoon.\\nStir in the shell pasta, and return to a boil. Cook uncovered, stirring occasionally, until the shells have cooked through, but are still firm to the bite, about 10 minutes. Spread the spinach over the top of the pasta, cover, and simmer until the spinach leaves are cooked, about 5 minutes. Sprinkle the mozzarella cheese evenly over the skillet, and simmer until the cheese has melted and the pasta is bubbling, about 5 minutes.\\n<|endofrecipe|>', \"<|startofrecipe|>Whole Grain Waffles<|startofingre|>1 1/4 cups all-purpose flour, 3/4 cup rolled oats, 1/4 cup firmly packed light brown sugar, 2 tablespoons wheat germ, 4 teaspoons baking powder, 1 teaspoon ground cinnamon, Pinch fine salt, 2 large eggs, 1 1/2 cups milk, 1/4 cup unsalted butter, melted, 1/4 cup peanut or walnut oil, Maple syrup, <|startofinstruc|>Preheat a waffle iron to medium-high.\\nWhisk the flour with the oats, sugar, wheat germ, baking powder, cinnamon, and salt in a medium bowl. In another medium bowl, lightly whisk the eggs, then add the milk, butter, and oil.\\nGently stir the wet ingredients into the dry ingredients with a wooden spoon, to make a batter. Take care not to overwork the batter, it's fine if there are a few lumps.\\nPour 1/3 to 1/2 cup of batter per waffle (it depends on the size of your waffle iron) and cook until the outside of the waffle is crisp and inside is cooked through, 3 to 5 minutes. (The time varies depending on the size and spread of a waffle iron). Serve warm with maple syrup. Repeat with remaining batter.<|endofrecipe|>\"]\n",
      "Number of train data:  80\n",
      "Number of test data:  21\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:52:22.159794Z",
     "start_time": "2024-06-26T12:52:21.625124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the GPT tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startofrecipe|>', eos_token='<|endofrecipe|>', pad_token='<|pad|>')\n",
    "# add special tokens for title, ingredients and instruction seperator\n",
    "special_tokens_dict = {'additional_special_tokens': ['<|startofingre|>', '<|startofinstruc|>']}\n",
    "# check the number of special tokens\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print('We have added', num_added_toks, 'tokens')"
   ],
   "id": "706ce9fabdb0d94d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 2 tokens\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:52:22.165844Z",
     "start_time": "2024-06-26T12:52:22.161112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.input_ids = []\n",
    "    self.attn_masks = []\n",
    "\n",
    "    for txt in txt_list:\n",
    "\n",
    "      encodings_dict = tokenizer(txt, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.attn_masks[idx] "
   ],
   "id": "8c47e16e0b206a69",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:52:22.297605Z",
     "start_time": "2024-06-26T12:52:22.168203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = GPT2Dataset(train_list, tokenizer, max_length=200)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ],
   "id": "e360975aa7958cbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   72 training samples\n",
      "    8 validation samples\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:52:22.302862Z",
     "start_time": "2024-06-26T12:52:22.299417Z"
    }
   },
   "cell_type": "code",
   "source": "batch_size = 2",
   "id": "4afb73b3e4cf22e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:52:22.307468Z",
     "start_time": "2024-06-26T12:52:22.304623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ],
   "id": "664664a1d0b96417",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:52:24.542024Z",
     "start_time": "2024-06-26T12:52:22.308552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ],
   "id": "7834e876e04b72f0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteorigat/PycharmProjects/nlp-project/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:52:24.546907Z",
     "start_time": "2024-06-26T12:52:24.543364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# some parameters I cooked up that work reasonably well\n",
    "\n",
    "epochs = 3\n",
    "learning_rate = 5e-4\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 1000\n",
    "# I save the model every 5000 step\n",
    "save_every = 5000\n",
    "# save the model to this file name\n",
    "save_file = 'trial_2'"
   ],
   "id": "fb436058765f5330",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:52:24.552966Z",
     "start_time": "2024-06-26T12:52:24.548455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = learning_rate,\n",
    "                  eps = epsilon\n",
    "                )"
   ],
   "id": "f91bed530dc8e2a5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:52:24.565080Z",
     "start_time": "2024-06-26T12:52:24.557530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "print('Total number of steps: ', total_steps)\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps, \n",
    "                                            num_training_steps = total_steps)"
   ],
   "id": "3e5bf04cbb506ff5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps:  108\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:54:54.899636Z",
     "start_time": "2024-06-26T12:52:24.567811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_stats = []\n",
    "print(\"Currently using device type: \", device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(  b_input_ids,\n",
    "                          labels=b_labels, \n",
    "                          attention_mask =b_masks,\n",
    "                          token_type_ids=None\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]  \n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        losses.append(batch_loss)\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "            print('Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.'.format(step, len(train_dataloader), batch_loss))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % save_every == 0:\n",
    "            model.save_pretrained(save_file)\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "    \n",
    "    # Calculate perplexity.\n",
    "    losses = torch.tensor(losses)\n",
    "    train_perplexity = math.exp(torch.mean(losses))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Perplexity: {0:.2f}\".format(train_perplexity))        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs  = model(b_input_ids, \n",
    "#                            token_type_ids=None, \n",
    "                             attention_mask = b_masks,\n",
    "                            labels=b_labels)\n",
    "          \n",
    "            loss = outputs[0]  \n",
    "            \n",
    "        batch_loss = loss.item()\n",
    "        losses.append(batch_loss)\n",
    "        total_eval_loss += batch_loss        \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Calculate perplexity.\n",
    "    losses = torch.tensor(losses)\n",
    "    val_perplexity = math.exp(torch.mean(losses))\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation perplexity: {0:.2f}\".format(val_perplexity))        \n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Perplexity': train_perplexity,\n",
    "            'Valid. Perplexity': val_perplexity,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ],
   "id": "7fad37e004a582de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using device type:  cpu\n",
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 18.58\n",
      "  Perplexity: 117179100.53\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 4.24\n",
      "  Validation perplexity: 69.29\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 3.50\n",
      "  Perplexity: 33.21\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 2.95\n",
      "  Validation perplexity: 19.17\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 2.54\n",
      "  Perplexity: 12.73\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 2.61\n",
      "  Validation perplexity: 13.54\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:54:55.248589Z",
     "start_time": "2024-06-26T12:54:54.901286Z"
    }
   },
   "cell_type": "code",
   "source": "model.save_pretrained(save_file)",
   "id": "ed06f23ccb2a4a41",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:54:55.278292Z",
     "start_time": "2024-06-26T12:54:55.249224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prepare datasets for dev_list and test_list\n",
    "test_dataset = GPT2Dataset(test_list, tokenizer, max_length=768)"
   ],
   "id": "6a5e950df7858e77",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:54:55.280882Z",
     "start_time": "2024-06-26T12:54:55.278955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the datasets\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ],
   "id": "b46acefc19d5a3d5",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:54:55.284622Z",
     "start_time": "2024-06-26T12:54:55.281604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, dataloaded):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    perplexity = []\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in dataloaded:\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs  = model(b_input_ids, \n",
    "    #                            token_type_ids=None, \n",
    "                            attention_mask = b_masks,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]  \n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        losses.append(batch_loss)\n",
    "        total_eval_loss += batch_loss        \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(dataloaded)\n",
    "\n",
    "    # Calculate perplexity.\n",
    "    losses = torch.tensor(losses)\n",
    "    val_perplexity = math.exp(torch.mean(losses))\n",
    "    perplexity.append(val_perplexity)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation perplexity: {0:.2f}\".format(val_perplexity))\n",
    "    return avg_val_loss, val_perplexity"
   ],
   "id": "30a38ad97ab7f901",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:55:15.807916Z",
     "start_time": "2024-06-26T12:54:55.285303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Testing...')\n",
    "test_loss, test_perplexity = evaluate_model(model, test_dataloader)\n",
    "test_eval_df = pd.DataFrame(columns = [\"test_loss\", \"test_perplexity\"])\n",
    "test_eval_df['test_loss'] = test_loss\n",
    "test_eval_df['test_perplexity'] = test_perplexity\n",
    "test_eval_df.to_csv(\"test_eval.csv\")"
   ],
   "id": "ced4489c324928de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "  Validation Loss: 0.93\n",
      "  Validation perplexity: 2.53\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:55:15.812390Z",
     "start_time": "2024-06-26T12:55:15.809685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the trained GPT-2 model and tokenizer\n",
    "#model_name = \"trial_2\"  # Path to the saved model\n",
    "#model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the model is on the right device\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)"
   ],
   "id": "a21baaa196c02c6d",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:55:15.817129Z",
     "start_time": "2024-06-26T12:55:15.813664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to generate a recipe from a list of ingredients\n",
    "def generate_recipe(ingredients, model, tokenizer, max_length=400):\n",
    "    # Prepare the input prompt with the list of ingredients\n",
    "    input_text = ingredients\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "    \n",
    "    # Generate the recipe\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the output to get the recipe text\n",
    "    recipe = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return recipe"
   ],
   "id": "4c714803a00ead9f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:55:19.834651Z",
     "start_time": "2024-06-26T12:55:15.818177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "ingredients = \"1 cup sugar, 2 cups flour, 1 teaspoon baking powder, 1/2 cup butter, 2 eggs, 1 cup milk\"\n",
    "generated_recipe = generate_recipe(ingredients, model, tokenizer)\n",
    "print(generated_recipe)"
   ],
   "id": "d6e39adac710fb12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 cup sugar, 2 cups flour, 1 teaspoon baking powder, 1/2 cup butter, 2 eggs, 1 cup milk, 3 tablespoons vanilla extract, Preheat oven to 350 degrees F (175 degrees C). Line a baking sheet with parchment paper and place in the oven. Bake for 10 to 12 minutes, or until golden brown, about 3 to 4 minutes.\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
