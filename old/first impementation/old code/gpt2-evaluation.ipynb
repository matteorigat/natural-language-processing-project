{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-26T15:47:15.529439Z",
     "start_time": "2024-06-26T15:47:15.523238Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matteorigat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:47:15.577050Z",
     "start_time": "2024-06-26T15:47:15.574811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# some parameters\n",
    "epochs = 3\n",
    "learning_rate = 1e-3\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "model_name = \"gpt2\"\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 1000\n",
    "# save the model every 5000 step\n",
    "save_every = 5000\n",
    "# save the model to this file name\n",
    "save_model = \"trial_2\""
   ],
   "id": "f2849cb9a12d679d",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:47:15.604454Z",
     "start_time": "2024-06-26T15:47:15.601160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load and also preprocess the raw data\n",
    "def load_preprocess_raw_data(raw_data):\n",
    "    recipe_instances = []\n",
    "\n",
    "    with open(raw_data, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            # Extract relevant fields from CSV row\n",
    "            #name = row['name'].lower().replace('\"', '')  # Remove any extra quotes\n",
    "            ingredients = row['ingredients'].lower().replace('\\'', '').replace('[', '').replace(']', '')\n",
    "            instructions = row['steps'].lower().replace('\\'', '').replace('[', '').replace(']', '')\n",
    "            \n",
    "            # Prepare recipe instance string\n",
    "            recipe_instance = '[BOS]'+ingredients+'[STEPS]'+instructions+'[EOS]' #+name+'[INGREDIENTS]'\n",
    "            \n",
    "            # Limit length to 2000 characters as per your function\n",
    "            if len(recipe_instance) <= 2000:\n",
    "                recipe_instances.append(recipe_instance)\n",
    "    \n",
    "    return recipe_instances"
   ],
   "id": "b013163b68b372",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:47:17.667603Z",
     "start_time": "2024-06-26T16:47:15.202545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create text list for dataset\n",
    "# https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions/data\n",
    "recipe_list = load_preprocess_raw_data(\"dataset/RAW_recipes.csv\")\n",
    "\n",
    "reduced_recipe_list = random.sample(recipe_list, int(0.002 * len(recipe_list)))\n",
    "print(reduced_recipe_list[:1])\n",
    "\n",
    "train_list, test_list = np.split(reduced_recipe_list, [int(.8*len(reduced_recipe_list))])\n",
    "print('\\nNumber of train data: ', len(train_list))\n",
    "print('Number of test data: ', len(test_list))"
   ],
   "id": "37ab75ee5d862da2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]vanilla ice cream, brandy, white creme de cacao, black coffee[STEPS]place all ingredients in a blender, blend on high speed until smooth, refrigerate at least 2 hours, \"dont skip this step !\", blend quickly on high just before serving[EOS]']\n",
      "\n",
      "Number of train data:  365\n",
      "Number of test data:  92\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:47:18.664096Z",
     "start_time": "2024-06-26T15:47:18.206048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the GPT tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, bos_token='[BOS]', eos_token='[EOS]', pad_token='[PAD]')\n",
    "# add special tokens for title, ingredients and instruction seperator\n",
    "special_tokens_dict = {'additional_special_tokens': ['[STEPS]']} #'[INGREDIENTS]', \n",
    "# check the number of special tokens\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print('We have added', num_added_toks, 'tokens')"
   ],
   "id": "706ce9fabdb0d94d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 1 tokens\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:47:18.668361Z",
     "start_time": "2024-06-26T15:47:18.664732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "  def __init__(self, txt_list, tokenizer, max_length=768):\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.input_ids = []\n",
    "    self.attn_masks = []\n",
    "\n",
    "    for txt in txt_list:\n",
    "        txt = self.custom_preprocessing(txt)\n",
    "        \n",
    "        encodings_dict = tokenizer(txt, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "    \n",
    "        self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "        self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "          \n",
    "  def custom_preprocessing(self, text):\n",
    "    # Example preprocessing: Lowercase the text and remove punctuation\n",
    "    text = text.lower()\n",
    "    # Add more preprocessing steps as needed\n",
    "    return text\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.attn_masks[idx] "
   ],
   "id": "8c47e16e0b206a69",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:47:18.876067Z",
     "start_time": "2024-06-26T15:47:18.669677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = GPT2Dataset(train_list, tokenizer, max_length=200)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ],
   "id": "e360975aa7958cbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  292 training samples\n",
      "   73 validation samples\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:47:18.878496Z",
     "start_time": "2024-06-26T15:47:18.876626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ],
   "id": "664664a1d0b96417",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:47:20.900598Z",
     "start_time": "2024-06-26T15:47:18.879090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained(model_name, output_hidden_states=False)\n",
    "\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed_val)"
   ],
   "id": "7834e876e04b72f0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteorigat/PycharmProjects/nlp-project/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:47:20.904357Z",
     "start_time": "2024-06-26T15:47:20.901851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = learning_rate,\n",
    "                  eps = epsilon\n",
    "                )"
   ],
   "id": "f91bed530dc8e2a5",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:47:20.992737Z",
     "start_time": "2024-06-26T15:47:20.905192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "print('Total number of steps: ', total_steps)\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps, \n",
    "                                            num_training_steps = total_steps)"
   ],
   "id": "3e5bf04cbb506ff5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps:  438\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:55:47.097092Z",
     "start_time": "2024-06-26T15:47:20.993763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_stats = []\n",
    "print(\"Currently using device type: \", device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    for step, batch in enumerate(loop):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(  b_input_ids,\n",
    "                          labels=b_labels, \n",
    "                          attention_mask =b_masks,\n",
    "                          token_type_ids=None\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]  \n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        losses.append(batch_loss)\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "            print('Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.'.format(step, len(train_dataloader), batch_loss))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % save_every == 0:\n",
    "            model.save_pretrained(save_model)\n",
    "            \n",
    "        loop.set_postfix(loss=batch_loss)\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "    \n",
    "    # Calculate perplexity.\n",
    "    losses = torch.tensor(losses)\n",
    "    train_perplexity = math.exp(torch.mean(losses))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Perplexity: {0:.2f}\".format(train_perplexity))        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs  = model(b_input_ids, \n",
    "#                            token_type_ids=None, \n",
    "                             attention_mask = b_masks,\n",
    "                            labels=b_labels)\n",
    "          \n",
    "            loss = outputs[0]  \n",
    "            \n",
    "        batch_loss = loss.item()\n",
    "        losses.append(batch_loss)\n",
    "        total_eval_loss += batch_loss        \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Calculate perplexity.\n",
    "    losses = torch.tensor(losses)\n",
    "    val_perplexity = math.exp(torch.mean(losses))\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation perplexity: {0:.2f}\".format(val_perplexity))        \n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Perplexity': train_perplexity,\n",
    "            'Valid. Perplexity': val_perplexity,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ],
   "id": "7fad37e004a582de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using device type:  cpu\n",
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [02:44<00:00,  1.13s/it, loss=2.9]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 4.96\n",
      "  Perplexity: 142.19\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 2.03\n",
      "  Validation perplexity: 7.58\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [02:39<00:00,  1.09s/it, loss=2.36] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 1.74\n",
      "  Perplexity: 5.67\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.90\n",
      "  Validation perplexity: 6.68\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [02:31<00:00,  1.04s/it, loss=1.93] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 1.45\n",
      "  Perplexity: 4.25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.89\n",
      "  Validation perplexity: 6.64\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:55:47.523184Z",
     "start_time": "2024-06-26T15:55:47.101257Z"
    }
   },
   "cell_type": "code",
   "source": "model.save_pretrained(save_model)",
   "id": "ed06f23ccb2a4a41",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:55:47.593133Z",
     "start_time": "2024-06-26T15:55:47.523876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prepare datasets for dev_list and test_list\n",
    "test_dataset = GPT2Dataset(test_list, tokenizer, max_length=768)"
   ],
   "id": "6a5e950df7858e77",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:55:47.596582Z",
     "start_time": "2024-06-26T15:55:47.593766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the datasets\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ],
   "id": "b46acefc19d5a3d5",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:55:47.600664Z",
     "start_time": "2024-06-26T15:55:47.597207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, dataloaded):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    perplexity = []\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in dataloaded:\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs  = model(b_input_ids, \n",
    "    #                            token_type_ids=None, \n",
    "                            attention_mask = b_masks,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]  \n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        losses.append(batch_loss)\n",
    "        total_eval_loss += batch_loss        \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(dataloaded)\n",
    "\n",
    "    # Calculate perplexity.\n",
    "    losses = torch.tensor(losses)\n",
    "    val_perplexity = math.exp(torch.mean(losses))\n",
    "    perplexity.append(val_perplexity)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation perplexity: {0:.2f}\".format(val_perplexity))\n",
    "    return avg_val_loss, val_perplexity"
   ],
   "id": "30a38ad97ab7f901",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:56:44.493796Z",
     "start_time": "2024-06-26T15:55:47.601506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Testing...')\n",
    "test_loss, test_perplexity = evaluate_model(model, test_dataloader)\n",
    "test_eval_df = pd.DataFrame(columns = [\"test_loss\", \"test_perplexity\"])\n",
    "test_eval_df['test_loss'] = test_loss\n",
    "test_eval_df['test_perplexity'] = test_perplexity\n",
    "test_eval_df.to_csv(\"test_eval.csv\")"
   ],
   "id": "ced4489c324928de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "  Validation Loss: 0.61\n",
      "  Validation perplexity: 1.84\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:56:44.497257Z",
     "start_time": "2024-06-26T15:56:44.495363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the trained GPT-2 model and tokenizer\n",
    "#model = GPT2LMHeadModel.from_pretrained(save_file)\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(save_file\n",
    "\n",
    "# Ensure the model is on the right device\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)"
   ],
   "id": "a21baaa196c02c6d",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:56:44.502282Z",
     "start_time": "2024-06-26T15:56:44.498648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to generate a recipe from a list of ingredients\n",
    "\"\"\"def generate_recipe(ingredients, model, tokenizer, max_length=400):\n",
    "    # Prepare the input prompt with the list of ingredients\n",
    "    input_text = ingredients\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "    \n",
    "    # Generate the recipe\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the output to get the recipe text\n",
    "    recipe = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return recipe\"\"\""
   ],
   "id": "4c714803a00ead9f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def generate_recipe(ingredients, model, tokenizer, max_length=400):\\n    # Prepare the input prompt with the list of ingredients\\n    input_text = ingredients\\n    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\\n    \\n    # Generate the recipe\\n    output = model.generate(\\n        input_ids,\\n        max_length=max_length,\\n        num_beams=5,\\n        no_repeat_ngram_size=2,\\n        num_return_sequences=1,\\n        pad_token_id=tokenizer.pad_token_id,\\n        eos_token_id=tokenizer.eos_token_id\\n    )\\n    \\n    # Decode the output to get the recipe text\\n    recipe = tokenizer.decode(output[0], skip_special_tokens=True)\\n    return recipe\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:57:24.938748Z",
     "start_time": "2024-06-26T16:57:24.934714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_recipe(ingredients, model, tokenizer, max_length=100, temperature=0.1, top_k=50, top_p=0.1):\n",
    "    input_text = '[BOS]' + ingredients + '[STEPS]'\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature, # Lower values make the model more confident (less random), while higher values increase randomness.\n",
    "        top_k=top_k,  #Increase to consider more tokens, decrease to restrict the model’s choices.\n",
    "        top_p=top_p,  # Increase to allow more diversity, decrease to make the model more conservative.\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    recipe = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Replace lowercase special tokens with uppercase\n",
    "    recipe = recipe.replace('[bos]', '[BOS]').replace('[steps]', '[STEPS]').replace('[eos]', '[EOS]')\n",
    "    \n",
    "    recipe = recipe.split('[EOS]', 1)[0] + '[EOS]'\n",
    "        \n",
    "    return recipe"
   ],
   "id": "8d333a467310cbb3",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:57:24.976343Z",
     "start_time": "2024-06-26T16:57:24.974473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_preprocessing(text):\n",
    "    # Example preprocessing: Lowercase the text and remove punctuation\n",
    "    text = text.lower()\n",
    "    #text = text.replace(\",\", \"\").replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace(\";\", \"\").replace(\"'\", \"\").replace('\"', \"\")\n",
    "    # Add more preprocessing steps as needed\n",
    "    return text"
   ],
   "id": "5cf44bf9144318f3",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:57:24.979661Z",
     "start_time": "2024-06-26T16:57:24.977486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_highlighted(generated_recipe, ingredients):\n",
    "    recipe=generated_recipe\n",
    "    ingredients_list = [ing.strip().lower() for ing in ingredients.split(',')]\n",
    "    for ingredient in ingredients_list:\n",
    "        recipe = recipe.replace(ingredient, f'\\033[91m{ingredient}\\033[0m')\n",
    "    return recipe"
   ],
   "id": "90b8ff971351a07c",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:57:29.114563Z",
     "start_time": "2024-06-26T16:57:25.026708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "ingredients = \"pasta, tomato, garlic, onion, olive oil, salt, pepper, basil, parmesan cheese\"\n",
    "#ingredients = custom_preprocessing(ingredients)\n",
    "\n",
    "generated_recipe = generate_recipe(ingredients, model, tokenizer)\n",
    "    \n",
    "print(print_highlighted(generated_recipe, ingredients))\n",
    "print(\"\\n\", len(generated_recipe) - len(ingredients))"
   ],
   "id": "d6e39adac710fb12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BOS]\u001B[91mflour\u001B[0m, \u001B[91msugar\u001B[0m, \u001B[91mcinnamon\u001B[0m, \u001B[91mcarrot\u001B[0m, \u001B[91mapple\u001B[0m, \u001B[91mwalnut\u001B[0m [STEPS] [PAD][BOS][PAD][PAD] \u001B[91mapple\u001B[0m cider vinegar[STEPS]combine the \u001B[91mflour\u001B[0m, \u001B[91msugar\u001B[0m, \u001B[91mcinnamon\u001B[0m, \u001B[91mcarrot\u001B[0m, \u001B[91mapple\u001B[0m, \u001B[91mwalnut\u001B[0ms, \u001B[91mapple\u001B[0msauce, and \u001B[91mapple\u001B[0m juice in a large bowl, mix well, pour into a greased 9x13-inch baking dish, bake at 350f for 30 minutes or until golden brown, remove from oven and let cool on wire racks, place the \u001B[91mapple\u001B[0m slices on a sheet pan and[EOS]\n",
      "\n",
      " 361\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:57:31.007017Z",
     "start_time": "2024-06-26T16:57:29.116208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rouge import Rouge\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Initialize models and tokenizers\n",
    "model_name_bert = 'bert-base-uncased'\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(model_name_bert)\n",
    "model_bert = BertModel.from_pretrained(model_name_bert)\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "# Function to calculate ROUGE-L F1 score\n",
    "def calculate_rouge_score(text1, text2):\n",
    "    scores = rouge.get_scores(text1, text2)\n",
    "    rouge_l_f1 = scores[0]['rouge-l']['f']\n",
    "    return rouge_l_f1\n",
    "\n",
    "# Function to get GPT-2 embeddings\n",
    "def get_gpt2_embedding(text, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    hidden_states = outputs[0]\n",
    "    pooled_embedding = torch.mean(hidden_states, dim=1)\n",
    "    return pooled_embedding\n",
    "\n",
    "# Function to calculate cosine similarity for GPT-2 embeddings\n",
    "def calculate_gpt2_similarity(text1, text2, model, tokenizer):\n",
    "    embedding1 = get_gpt2_embedding(text1, model, tokenizer)\n",
    "    embedding2 = get_gpt2_embedding(text2, model, tokenizer)\n",
    "    similarity = cosine_similarity(embedding1, embedding2).item()\n",
    "    return similarity\n",
    "\n",
    "# Function to encode text for BERT\n",
    "def encode_text(text, tokenizer):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    return input_ids\n",
    "\n",
    "# Function to calculate BERT embeddings\n",
    "def get_bert_embedding(input_ids, model):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        pooled_embedding = torch.mean(last_hidden_state, dim=1)\n",
    "    return pooled_embedding\n",
    "\n",
    "# Function to calculate cosine similarity for BERT embeddings\n",
    "def calculate_bert_similarity(text1, text2, model, tokenizer):\n",
    "    input1 = encode_text(text1, tokenizer)\n",
    "    input2 = encode_text(text2, tokenizer)\n",
    "    embedding1 = get_bert_embedding(input1, model)\n",
    "    embedding2 = get_bert_embedding(input2, model)\n",
    "    similarity = cosine_similarity(embedding1.cpu(), embedding2.cpu()).item()\n",
    "    return similarity\n",
    "\n",
    "# Function to evaluate generated recipe against a list of real recipes\n",
    "def evaluate_generated_recipe(generated_recipe, real_recipes):\n",
    "    rouge_scores = []\n",
    "    gpt2_similarities = []\n",
    "    bert_similarities = []\n",
    "\n",
    "    for real_recipe in real_recipes:\n",
    "        rouge_score = calculate_rouge_score(generated_recipe, real_recipe)\n",
    "        gpt2_similarity = calculate_gpt2_similarity(generated_recipe, real_recipe, model, tokenizer)\n",
    "        bert_similarity = calculate_bert_similarity(generated_recipe, real_recipe, model_bert, tokenizer_bert)\n",
    "\n",
    "        rouge_scores.append(rouge_score)\n",
    "        gpt2_similarities.append(gpt2_similarity)\n",
    "        bert_similarities.append(bert_similarity)\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_scores = [(sum(scores) / len(scores)) for scores in zip(rouge_scores)]\n",
    "    #, gpt2_similarities, bert_similarities\n",
    "\n",
    "    # Find index of recipe with maximum average score\n",
    "    max_index = avg_scores.index(max(avg_scores))\n",
    "\n",
    "    return real_recipes[max_index], rouge_scores[max_index], gpt2_similarities[max_index], bert_similarities[max_index]"
   ],
   "id": "89ac742c3f3d583c",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:57:31.014865Z",
     "start_time": "2024-06-26T16:57:31.011906Z"
    }
   },
   "cell_type": "code",
   "source": "#reduced_recipe_list2 = [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\",\"Start with a ball of pizza dough, roll it out into a thin crust. Spread a layer of tomato sauce evenly over the dough. Add a generous amount of shredded mozzarella cheese on top. Optionally, sprinkle with dried oregano and a pinch of salt. Preheat your oven to a high temperature, around 450°F (230°C). Place the pizza on a baking sheet or pizza stone and bake for 10-15 minutes, or until the crust is golden brown and the cheese is bubbly. Remove from the oven, let it cool slightly, then slice and enjoy your delicious homemade pizza!\"]",
   "id": "a0d13c442f0bb873",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:00:07.814619Z",
     "start_time": "2024-06-26T16:57:31.016767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_recipe = evaluate_generated_recipe(generated_recipe, reduced_recipe_list)\n",
    "\n",
    "print(\"Generated Recipe:\")\n",
    "print(print_highlighted(generated_recipe, ingredients))\n",
    "print(\"\\nMost Similar Real Recipe:\")\n",
    "print(print_highlighted(best_recipe[0], ingredients), \"\\n\\nrouge-l f1:\", best_recipe[1], \"\\nGPT-2 similarity:\", best_recipe[2], \"\\nBERT similarity:\", best_recipe[3])"
   ],
   "id": "4344e49b4dedd268",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Recipe:\n",
      "[BOS]\u001B[91mflour\u001B[0m, \u001B[91msugar\u001B[0m, \u001B[91mcinnamon\u001B[0m, \u001B[91mcarrot\u001B[0m, \u001B[91mapple\u001B[0m, \u001B[91mwalnut\u001B[0m [STEPS] [PAD][BOS][PAD][PAD] \u001B[91mapple\u001B[0m cider vinegar[STEPS]combine the \u001B[91mflour\u001B[0m, \u001B[91msugar\u001B[0m, \u001B[91mcinnamon\u001B[0m, \u001B[91mcarrot\u001B[0m, \u001B[91mapple\u001B[0m, \u001B[91mwalnut\u001B[0ms, \u001B[91mapple\u001B[0msauce, and \u001B[91mapple\u001B[0m juice in a large bowl, mix well, pour into a greased 9x13-inch baking dish, bake at 350f for 30 minutes or until golden brown, remove from oven and let cool on wire racks, place the \u001B[91mapple\u001B[0m slices on a sheet pan and[EOS]\n",
      "\n",
      "Most Similar Real Recipe:\n",
      "[BOS]stale bread, eggs, \u001B[91msugar\u001B[0m, butter, crushed pine\u001B[91mapple\u001B[0m, salt[STEPS]cream \u001B[91msugar\u001B[0m margarine or butter , salt and eggs, stir in pine\u001B[91mapple\u001B[0m and fold in bread, pour in greased casserole, bake , uncovered at 350f for 45 minutes or until crispy[EOS] \n",
      "\n",
      "rouge-l f1: 0.28915662182609964 \n",
      "GPT-2 similarity: 0.9977008104324341 \n",
      "BERT similarity: 0.9282747507095337\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:00:07.821969Z",
     "start_time": "2024-06-26T17:00:07.816015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Function to extract ingredients from a recipe\n",
    "def extract_ingredients(recipe):\n",
    "    start = recipe.find('[BOS]') + len('[BOS]')\n",
    "    end = recipe.find('[STEPS]')\n",
    "    ingredients = recipe[start:end].strip()\n",
    "    return ingredients\n",
    "\n",
    "# Function to calculate cosine similarity for ingredient lists\n",
    "def calculate_ingredient_similarity(ingredients1, ingredients2):\n",
    "    vectorizer = TfidfVectorizer().fit_transform([ingredients1, ingredients2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_sim = cosine_similarity(vectors)\n",
    "    return cosine_sim[0, 1]\n",
    "\n",
    "# Function to evaluate generated recipe against a list of real recipes\n",
    "def evaluate_generated_recipe_by_ingredients(generated_recipe, real_recipes, top_k=5):\n",
    "    generated_ingredients = extract_ingredients(generated_recipe)\n",
    "    \n",
    "    similarities = []\n",
    "    for real_recipe in real_recipes:\n",
    "        real_ingredients = extract_ingredients(real_recipe)\n",
    "        similarity = calculate_ingredient_similarity(generated_ingredients, real_ingredients)\n",
    "        similarities.append((real_recipe, similarity))\n",
    "    \n",
    "    # Sort recipes based on ingredient similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top k recipes\n",
    "    top_k_recipes = similarities[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for real_recipe, sim in top_k_recipes:\n",
    "        rouge_score = calculate_rouge_score(generated_recipe, real_recipe)\n",
    "        gpt2_similarity = calculate_gpt2_similarity(generated_recipe, real_recipe, model, tokenizer)\n",
    "        bert_similarity = calculate_bert_similarity(generated_recipe, real_recipe, model_bert, tokenizer_bert)\n",
    "        results.append((real_recipe, sim, rouge_score, gpt2_similarity, bert_similarity))\n",
    "    \n",
    "    return results"
   ],
   "id": "11589f46596dac8e",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:00:09.767345Z",
     "start_time": "2024-06-26T17:00:07.822969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate and print top k recipes\n",
    "top_k = 5\n",
    "top_k_recipes = evaluate_generated_recipe_by_ingredients(generated_recipe, reduced_recipe_list, top_k=top_k)\n",
    "\n",
    "for i, (recipe, ingredient_sim, rouge_score, gpt2_sim, bert_sim) in enumerate(top_k_recipes):\n",
    "    print(f\"\\nRecipe {i+1} (Ingredient Similarity: {ingredient_sim:.2f}):\")\n",
    "    print(print_highlighted(recipe, ingredients))\n",
    "    print(f\"ROUGE-L F1: {rouge_score:.4f}\")\n",
    "    print(f\"GPT-2 Similarity: {gpt2_sim:.4f}\")\n",
    "    print(f\"BERT Similarity: {bert_sim:.4f}\")"
   ],
   "id": "45d2d0a1cc736a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recipe 1 (Ingredient Similarity: 0.36):\n",
      "[BOS]\u001B[91mapple\u001B[0ms, \u001B[91mcinnamon\u001B[0m, water, butter, \u001B[91mflour\u001B[0m, \u001B[91msugar\u001B[0m, brown \u001B[91msugar\u001B[0m[STEPS]preheat oven to 350, grease 8 x 8 dish, peel and cut \u001B[91mapple\u001B[0ms, place \u001B[91mapple\u001B[0ms in pan, add \u001B[91mcinnamon\u001B[0m and water, separately , mix melted butter , \u001B[91mflour\u001B[0m and \u001B[91msugar\u001B[0ms, spread evenly and firmly on top of \u001B[91mapple\u001B[0m mixture, bake 45-50 minutes[EOS]\n",
      "ROUGE-L F1: 0.1684\n",
      "GPT-2 Similarity: 0.9992\n",
      "BERT Similarity: 0.9510\n",
      "\n",
      "Recipe 2 (Ingredient Similarity: 0.30):\n",
      "[BOS]sour cream, butter, egg, bran flakes, \u001B[91mflour\u001B[0m, \u001B[91msugar\u001B[0m, \u001B[91mcinnamon\u001B[0m, baking soda, salt, \u001B[91mapple\u001B[0m[STEPS]preheat oven to 375, grease muffin tin, combine sour cream , butter and egg, add bran flakes , let stand till softened, add combined dry ingredients , mixing till just moistened, fold in \u001B[91mapple\u001B[0ms, spoon into greased muffin cups , filling cups 3 / 4 full, bake at 375 25 minutes[EOS]\n",
      "ROUGE-L F1: 0.1698\n",
      "GPT-2 Similarity: 0.9990\n",
      "BERT Similarity: 0.9434\n",
      "\n",
      "Recipe 3 (Ingredient Similarity: 0.25):\n",
      "[BOS]rome \u001B[91mapple\u001B[0ms, unsalted butter, lemon, juice and zest of, dark brown \u001B[91msugar\u001B[0m, raisins, \u001B[91mwalnut\u001B[0m pieces, \u001B[91mcinnamon\u001B[0m, \u001B[91mapple\u001B[0m cider[STEPS]preheat oven to 350 degrees, peel about 1 / 2 inch of skin from tops of \u001B[91mapple\u001B[0ms with a vegetable peeler, cut a sliver off of the bottom of each \u001B[91mapple\u001B[0m so they will stand, brush the peeled part and the interior of each \u001B[91mapple\u001B[0m with lemon juice, cut 1 tbsp of butter into 4 pieces and place a piece of it in each \u001B[91mapple\u001B[0m, in a bowl , combine the lemon zest , remaining butter , brown \u001B[91msugar\u001B[0m , raisins , \u001B[91mwalnut\u001B[0ms and \u001B[91mcinnamon\u001B[0m, stuff \u001B[91mapple\u001B[0ms with mixture and place them in a small baking dish, add cider and bake for 30-35 minutes or until tender, serve with whipped cream or ice cream[EOS]\n",
      "ROUGE-L F1: 0.1972\n",
      "GPT-2 Similarity: 0.9993\n",
      "BERT Similarity: 0.9387\n",
      "\n",
      "Recipe 4 (Ingredient Similarity: 0.23):\n",
      "[BOS]cranberries, \u001B[91mflour\u001B[0m, salt, double-acting baking powder, butter, milk, \u001B[91msugar\u001B[0m, \u001B[91mcinnamon\u001B[0m[STEPS]place the cranberries in a saucepan , cover with water , and simmer until the berries have burst and the water is colored, drain , reserving the liquid, set both aside, in a large bowl , mix together the sifted \u001B[91mflour\u001B[0m , salt , and baking powder, with a pastry blender or a couple of knives , cut in the chilled butter until the mixture is a bit smaller than pea-sized, make a well in the middle of the mixture , then pour in the milk and quickly stir it until it just sticks together to form a dough, turn the dough out onto a lightly \u001B[91mflour\u001B[0med board and knead gently for no more than 10 times, wrap the dough loosely in waxed paper and chill it in the refrigerator for at least 30 minutes to make it easier to handle, about 60 minutes before you intend to serve the dumplings , roll out the dough on a lightly \u001B[91mflour\u001B[0med board until it is about 1 / 4-inch thick , and cut into 4-inch squares, fill the center of each square with a bit of the cooked cranberries and sprinkle each generously with \u001B[91msugar\u001B[0m and \u001B[91mcinnamon\u001B[0m to taste, wet the edges with a small amount of water , fold the dumplings over and pinch them closed, prick the tops, re-heat the reserved cranberry juice from the berry cooking in a saucepan to simmering, cook the dumplings in the juice for about 15 minutes until done, if you dont have enough liquid from cooking the berries , you can augment with commercially prepared unsweetened cranberry juice with water added until it tastes like the cooking liquid, goes really well with duck , pork , and turkey dishes, if you use them , the cooking time for the dumplings will be less[EOS]\n",
      "ROUGE-L F1: 0.1204\n",
      "GPT-2 Similarity: 0.9986\n",
      "BERT Similarity: 0.8991\n",
      "\n",
      "Recipe 5 (Ingredient Similarity: 0.23):\n",
      "[BOS]soymilk, \u001B[91mcinnamon\u001B[0m, brown \u001B[91msugar\u001B[0m, oatmeal[STEPS]warm milk in the microwave for 1 minute, put the oats , \u001B[91msugar\u001B[0m and \u001B[91mcinnamon\u001B[0m and \u001B[91msugar\u001B[0m in and stir, let stand for 1-2 minutes, eat, you can also add \u001B[91mapple\u001B[0m slices on top , raisins , nuts, go crazy[EOS]\n",
      "ROUGE-L F1: 0.1818\n",
      "GPT-2 Similarity: 0.9985\n",
      "BERT Similarity: 0.9072\n"
     ]
    }
   ],
   "execution_count": 92
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
