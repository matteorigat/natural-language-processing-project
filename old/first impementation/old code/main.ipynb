{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-20T16:38:05.955602Z",
     "start_time": "2024-06-20T16:38:05.946660Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T16:38:08.370708Z",
     "start_time": "2024-06-20T16:38:05.968121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('dataset/RAW_recipes.csv')\n",
    "#print(data.head())\n",
    "sample_size = int(len(data) * 0.001)\n",
    "data_sample = data.sample(n=sample_size)\n",
    "ingredients = data_sample['ingredients']\n",
    "instructions = data_sample['steps']\n",
    "print(ingredients.head())\n",
    "print(instructions.head())"
   ],
   "id": "28ea1388d4a7d155",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156467    ['spaghetti', 'broccoli florets', 'chunky pean...\n",
      "147896    ['olive oil', 'spicy sausage', 'onion', 'carro...\n",
      "133179    ['halibut', 'fresh lemon juice', 'olive oil', ...\n",
      "125385    ['water', 'sugar', 'orange, zest of', 'lemons,...\n",
      "9287      ['salmon fillets', 'dijon mustard', 'soy sauce...\n",
      "Name: ingredients, dtype: object\n",
      "156467    ['bring a large pot water to boil', 'cook past...\n",
      "147896    ['heat oil in a skillet over medium heat and f...\n",
      "133179    ['mix halibut , lemon juice and 4 teaspoons oi...\n",
      "125385    ['bring water and sugar to boil and stir to di...\n",
      "9287      ['heat grill and brush rack with olive oil to ...\n",
      "Name: steps, dtype: object\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T16:38:10.313803Z",
     "start_time": "2024-06-20T16:38:08.372142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ],
   "id": "c43f9dd7eb292cb3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T16:38:10.412190Z",
     "start_time": "2024-06-20T16:38:10.315166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare the data for training\n",
    "inputs = tokenizer(ingredients.tolist(), return_tensors='pt', truncation=True, padding=True)\n",
    "labels = torch.tensor([1]*len(ingredients))  # assuming all recipes are \"positive\""
   ],
   "id": "6b14b622f39f64c0",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T16:38:10.498075Z",
     "start_time": "2024-06-20T16:38:10.413744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=1e-5) "
   ],
   "id": "deb4c8320b8fd64d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T16:38:45.970635Z",
     "start_time": "2024-06-20T16:38:10.498911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fine-tune the model\n",
    "model.train()\n",
    "num_epochs = 1  # Define the number of epochs\n",
    "batch_size = 32  # Define the batch size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(DataLoader(list(zip(inputs['input_ids'], inputs['attention_mask'], labels)), batch_size=batch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, label_batch = batch\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=label_batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "id": "fce9f407535efa01",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T16:54:22.364580Z",
     "start_time": "2024-06-20T16:54:22.273573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer.encode('chicken, rice, carrots', return_tensors='pt')\n",
    "generated = model.generate(input_ids, max_length=100, do_sample=True)\n",
    "recipe = tokenizer.decode(generated[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "print(recipe)"
   ],
   "id": "d93e336bbd277dd1",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchicken, rice, carrots\u001B[39m\u001B[38;5;124m'\u001B[39m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m generated \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(input_ids, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, do_sample\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      3\u001B[0m recipe \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(generated[:, input_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]:][\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
