{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:45.036846Z",
     "start_time": "2024-10-08T09:31:41.450472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import get_linear_schedule_with_warmup\n"
   ],
   "id": "19ca7bc8df5d6f71",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:45.042441Z",
     "start_time": "2024-10-08T09:31:45.040253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# some parameters\n",
    "epochs = 1\n",
    "learning_rate = 5e-5  #default 1e-3 5e-5\n",
    "epsilon = 1e-8  #default\n",
    "model_name = \"gpt2\"\n",
    "batch_size = 4\n",
    "\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 100\n",
    "# save the model every 1000 step\n",
    "save_every = 1000\n",
    "# save the model to this file name\n",
    "save_model = \"models/topsmallest_epochs\"\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize, pos_tag\n",
    "import re\n",
    "\n",
    "# Ensure NLTK downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess_ingredients(ingredients):\n",
    "    ingredients_list = eval(ingredients)\n",
    "    processed_ingredients = []\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #stemmer = PorterStemmer()\n",
    "\n",
    "    # POS tags that represent nouns\n",
    "    noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "    # Define the words to be dropped\n",
    "    #words_to_drop = {\"powder\", \"brown\", \"salt\", \"water\", \"sugar\", \"onion\", \"butter\", \"pepper\", \"ground\", \"cream\"}\n",
    "\n",
    "    for ingr in ingredients_list:\n",
    "        ingr = regex.sub(' ', ingr.lower()).strip()\n",
    "        components = [comp.strip() for comp in ingr.split('and')]\n",
    "\n",
    "        for comp in components:\n",
    "\n",
    "            sentence = \"\"\n",
    "            tokens = word_tokenize(comp)  # Tokenize each component\n",
    "            tagged_tokens = pos_tag(tokens)  # Perform POS tagging\n",
    "\n",
    "            # Extract main nouns while handling compound nouns\n",
    "            nouns = []\n",
    "            current_noun = \"\"\n",
    "            for word, tag in tagged_tokens:\n",
    "                word = lemmatizer.lemmatize(word.strip())\n",
    "                if len(word) > 2 and word not in stop_words and tag in noun_tags:  # and word not in words_to_drop\n",
    "                    if current_noun:\n",
    "                        nouns.append(current_noun)\n",
    "                        current_noun = \"\"\n",
    "                    current_noun = word\n",
    "\n",
    "            # Add last current noun if exists\n",
    "            if current_noun:\n",
    "                nouns.append(current_noun)\n",
    "\n",
    "            for word in nouns:\n",
    "                singular_comp = lemmatizer.lemmatize(word.strip())\n",
    "                #stemmed_word = stemmer.stem(singular_comp)\n",
    "\n",
    "                if singular_comp not in stop_words and len(singular_comp) > 2:\n",
    "                    sentence += singular_comp + \" \"\n",
    "\n",
    "            if sentence.strip():\n",
    "                processed_ingredients.append(sentence.strip())\n",
    "\n",
    "    return list(set(processed_ingredients))\n",
    "\n",
    "\n",
    "# Funzione di preprocessing per le tecniche\n",
    "def preprocess_techniques(techniques):\n",
    "    techniques_list = eval(techniques)\n",
    "    processed_techniques = []\n",
    "\n",
    "    for technique in techniques_list:\n",
    "        technique = technique.lower().strip()\n",
    "        tokens = word_tokenize(technique)\n",
    "        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "        processed_techniques.append(' '.join(tokens))\n",
    "\n",
    "    return processed_techniques\n",
    "\n",
    "\n",
    "# Funzione per preprocessare ogni riga della ricetta\n",
    "def process_recipe(row):\n",
    "    ingredients = preprocess_ingredients(row['ingredients'])\n",
    "    #ingredients = preprocess_techniques(row['ingredients'])\n",
    "    techniques = preprocess_techniques(row['techniques_list'])\n",
    "    instructions = row['steps'].lower().replace('\\'', '').replace('[', '').replace(']', '').replace('\"', '')\n",
    "\n",
    "    ingredients_str = ''.join([f\", {ingr}\" for ingr in ingredients])\n",
    "    ingredients_str = ingredients_str.replace(\", \", \"\", 1)\n",
    "\n",
    "    techniques_str = ''.join([f\", {tec}\" for tec in techniques])\n",
    "    techniques_str = techniques_str.replace(\", \", \"\", 1)\n",
    "\n",
    "    recipe_instance = f\"[BOS] [INGREDIENTS] {ingredients_str} [TECHNIQUES] {techniques_str} [STEPS] {instructions} [EOS]\"\n",
    "    return recipe_instance\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Funzione per preprocessare i dati in parallelo\n",
    "def load_preprocess_raw_data_parallel(raw_data):\n",
    "    with open(raw_data, 'r', encoding='utf-8') as f:\n",
    "        reader = list(csv.DictReader(f))  # Convertiamo reader in una lista\n",
    "        num_cores = multiprocessing.cpu_count()  # Otteniamo il numero di core della CPU\n",
    "        print(f\"Number of CPU cores: {num_cores}\")\n",
    "\n",
    "        # Eseguiamo il preprocessing in parallelo\n",
    "        recipe_instances = Parallel(n_jobs=num_cores)(\n",
    "            delayed(process_recipe)(row) for row in tqdm(reader, desc=\"Processing recipes\", unit=\"recipes\")\n",
    "        )\n",
    "\n",
    "    return recipe_instances\n",
    "\n",
    "\n",
    "# create text list for dataset\n",
    "# https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions/data\n",
    "#recipe_list = load_preprocess_raw_data(\"dataset/RAW_merged.csv\")\n",
    "recipe_list = load_preprocess_raw_data_parallel(\"dataset/RAW_merged_top_smallest.csv\")\n",
    "recipe_list = recipe_list[:100]\n",
    "\n",
    "train_list, test_list = np.split(recipe_list, [int(.8 * len(recipe_list))])\n",
    "print('\\nNumber of train data: ', len(train_list))\n",
    "print('Number of test data: ', len(test_list))\n",
    "print(train_list[0])\n",
    "# Load the GPT tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, bos_token='[BOS]', eos_token='[EOS]', pad_token='[PAD]')\n",
    "# add special tokens for title, ingredients and instruction seperator\n",
    "special_tokens_dict = {'additional_special_tokens': ['[INGREDIENTS]', '[TECHNIQUES]', '[STEPS]']}  # '[INGR]', '[STEP]'\n",
    "# check the number of special tokens\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "# Verifica gli ID dei token speciali\n",
    "special_tokens = ['[BOS]', '[EOS]', '[PAD]', '[INGREDIENTS]', '[TECHNIQUES]', '[STEPS]']  # '[INGR]'\n",
    "for token in special_tokens:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"Token: {token}, ID: {token_id}\")\n",
    "# Esempio di testi\n",
    "texts = [\n",
    "    train_list[0]\n",
    "]\n",
    "\n",
    "# Tokenizzazione\n",
    "encodings = tokenizer.batch_encode_plus(\n",
    "    texts,\n",
    "    truncation=True,\n",
    "    max_length=320,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt',\n",
    "    add_special_tokens=True  # Assicurati che i token speciali siano inclusi\n",
    ")\n",
    "\n",
    "# Visualizza gli input_ids e attention_mask\n",
    "print(\"Input IDs:\")\n",
    "print(encodings['input_ids'])\n",
    "print(\"\\nAttention Mask:\")\n",
    "print(encodings['attention_mask'])\n",
    "# Decodifica dei token\n",
    "decoded_inputs = [tokenizer.decode(ids, skip_special_tokens=False) for ids in encodings['input_ids']]\n",
    "for i, decoded in enumerate(decoded_inputs):\n",
    "    print(f\"Decoded Input {i}: {decoded}\")\n",
    "lengths = [len(tokenizer.encode(recipe)) for recipe in recipe_list]\n",
    "max_length_in_data = max(lengths)\n",
    "avg_length_in_data = sum(lengths) / len(lengths)\n",
    "print(f\"Lunghezza massima: {max_length_in_data}, Lunghezza media: {avg_length_in_data}\")\n",
    "\n",
    "\n",
    "# Lunghezza massima: 312, Lunghezza media: 136.59567387687187\n",
    "class GPT2Dataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length=320):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.texts = [text.lower() for text in txt_list]  # Preprocess texts to lowercase\n",
    "\n",
    "        # Tokenize all texts in a batch\n",
    "        encodings = tokenizer.batch_encode_plus(\n",
    "            self.texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        self.input_ids = encodings['input_ids']\n",
    "        self.attn_masks = encodings['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n",
    "\n",
    "dataset = GPT2Dataset(train_list, tokenizer, max_length=320)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order.\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,  # The training samples.\n",
    "    sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
    "    batch_size=batch_size  # Trains with this batch size.\n",
    ")\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,  # The validation samples.\n",
    "    sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
    "    batch_size=batch_size  # Evaluate with this batch size.\n",
    ")\n",
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained(model_name, output_hidden_states=False)\n",
    "\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=learning_rate,\n",
    "                              eps=epsilon\n",
    "                              )\n",
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "print('Total number of steps: ', total_steps)\n",
    "\n",
    "warmup_steps = int(0.05 * total_steps)  # 5% del totale degli step\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)\n",
    "training_stats = []\n",
    "print(\"Currently using device type: \", device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    for step, batch in enumerate(loop):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids,\n",
    "                        labels=b_labels,\n",
    "                        attention_mask=b_masks\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        losses.append(batch_loss)\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "            print('Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.'.format(step, len(train_dataloader), batch_loss))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % save_every == 0:\n",
    "            model.save_pretrained(save_model)\n",
    "\n",
    "        loop.set_postfix(loss=batch_loss)\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Calculate perplexity.\n",
    "    losses = torch.tensor(losses)\n",
    "    train_perplexity = math.exp(torch.mean(losses))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Perplexity: {0:.2f}\".format(train_perplexity))\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids,\n",
    "                            attention_mask=b_masks,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        losses.append(batch_loss)\n",
    "        total_eval_loss += batch_loss\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    # Calculate perplexity.\n",
    "    losses = torch.tensor(losses)\n",
    "    val_perplexity = math.exp(torch.mean(losses))\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation perplexity: {0:.2f}\".format(val_perplexity))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Perplexity': train_perplexity,\n",
    "            'Valid. Perplexity': val_perplexity,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "#model.save_pretrained(save_model)\n",
    "# # prepare datasets for dev_list and test_list\n",
    "# test_dataset = GPT2Dataset(test_list, tokenizer, max_length=320)\n",
    "# # load the datasets\n",
    "# test_dataloader = DataLoader(\n",
    "#     test_dataset,  # The validation samples.\n",
    "#     sampler=SequentialSampler(test_dataset),  # Pull out batches sequentially.\n",
    "#     batch_size=batch_size  # Evaluate with this batch size.\n",
    "# )\n",
    "# def evaluate_model(model, dataloaded):\n",
    "#     model = model.to(device)\n",
    "#     model.eval()\n",
    "#\n",
    "#     losses = []\n",
    "#     perplexity = []\n",
    "#     total_eval_loss = 0\n",
    "#\n",
    "#     # Evaluate data for one epoch\n",
    "#     for batch in dataloaded:\n",
    "#         b_input_ids = batch[0].to(device)\n",
    "#         b_labels = batch[0].to(device)\n",
    "#         b_masks = batch[1].to(device)\n",
    "#\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(b_input_ids,\n",
    "#                             attention_mask=b_masks,\n",
    "#                             labels=b_labels)\n",
    "#\n",
    "#             loss = outputs[0]\n",
    "#\n",
    "#         batch_loss = loss.item()\n",
    "#         losses.append(batch_loss)\n",
    "#         total_eval_loss += batch_loss\n",
    "#\n",
    "#     avg_val_loss = total_eval_loss / len(dataloaded)\n",
    "#\n",
    "#     # Calculate perplexity.\n",
    "#     losses = torch.tensor(losses)\n",
    "#     val_perplexity = math.exp(torch.mean(losses))\n",
    "#     perplexity.append(val_perplexity)\n",
    "#\n",
    "#     print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "#     print(\"  Validation perplexity: {0:.2f}\".format(val_perplexity))\n",
    "#     return avg_val_loss, val_perplexity\n",
    "# print('Testing...')\n",
    "# test_loss, test_perplexity = evaluate_model(model, test_dataloader)\n",
    "# test_eval_df = pd.DataFrame(columns=[\"test_loss\", \"test_perplexity\"])\n",
    "# test_eval_df['test_loss'] = test_loss\n",
    "# test_eval_df['test_perplexity'] = test_perplexity\n",
    "# test_eval_df.to_csv(\"test_eval.csv\")\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 100\n",
    "# save the model every 1000 step\n",
    "save_every = 1000\n",
    "# save the model to this file name\n",
    "save_model = \"models/topsmallest_epochs\""
   ],
   "id": "fce95e3efb4faf30",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:46.484510Z",
     "start_time": "2024-10-08T09:31:45.253199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize, pos_tag\n",
    "import re\n",
    "\n",
    "# Ensure NLTK downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_ingredients(ingredients):\n",
    "    ingredients_list = eval(ingredients)\n",
    "    processed_ingredients = []\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #stemmer = PorterStemmer()\n",
    "    \n",
    "    # POS tags that represent nouns\n",
    "    noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    \n",
    "    # Define the words to be dropped\n",
    "    #words_to_drop = {\"powder\", \"brown\", \"salt\", \"water\", \"sugar\", \"onion\", \"butter\", \"pepper\", \"ground\", \"cream\"} \n",
    "\n",
    "    for ingr in ingredients_list:\n",
    "        ingr = regex.sub(' ', ingr.lower()).strip()\n",
    "        components = [comp.strip() for comp in ingr.split('and')]\n",
    "        \n",
    "\n",
    "        for comp in components:\n",
    "                        \n",
    "            sentence = \"\"\n",
    "            tokens = word_tokenize(comp)  # Tokenize each component\n",
    "            tagged_tokens = pos_tag(tokens)  # Perform POS tagging\n",
    "            \n",
    "            # Extract main nouns while handling compound nouns\n",
    "            nouns = []\n",
    "            current_noun = \"\"\n",
    "            for word, tag in tagged_tokens:\n",
    "                word = lemmatizer.lemmatize(word.strip())\n",
    "                if len(word) > 2 and word not in stop_words and tag in noun_tags: # and word not in words_to_drop\n",
    "                    if current_noun:\n",
    "                        nouns.append(current_noun)\n",
    "                        current_noun = \"\"\n",
    "                    current_noun = word\n",
    "            \n",
    "            # Add last current noun if exists\n",
    "            if current_noun:\n",
    "                nouns.append(current_noun)            \n",
    "            \n",
    "            for word in nouns:\n",
    "                singular_comp = lemmatizer.lemmatize(word.strip())\n",
    "                #stemmed_word = stemmer.stem(singular_comp)\n",
    "            \n",
    "                if singular_comp not in stop_words and len(singular_comp) > 2:\n",
    "                    sentence += singular_comp + \" \"\n",
    "                    \n",
    "            if sentence.strip():\n",
    "                processed_ingredients.append(sentence.strip())\n",
    "\n",
    "    return list(set(processed_ingredients))\n",
    "\n",
    "# Funzione di preprocessing per le tecniche\n",
    "def preprocess_techniques(techniques):\n",
    "    techniques_list = eval(techniques)\n",
    "    processed_techniques = []\n",
    "\n",
    "    for technique in techniques_list:\n",
    "        technique = technique.lower().strip()\n",
    "        tokens = word_tokenize(technique)\n",
    "        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "        processed_techniques.append(' '.join(tokens))\n",
    "\n",
    "    return processed_techniques"
   ],
   "id": "f39a9f740aad3497",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matteorigat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/matteorigat/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/matteorigat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matteorigat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/matteorigat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:46.492532Z",
     "start_time": "2024-10-08T09:31:46.490129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Funzione per preprocessare ogni riga della ricetta\n",
    "def process_recipe(row):\n",
    "    ingredients = preprocess_ingredients(row['ingredients'])\n",
    "    #ingredients = preprocess_techniques(row['ingredients'])\n",
    "    techniques = preprocess_techniques(row['techniques_list'])\n",
    "    instructions = row['steps'].lower().replace('\\'', '').replace('[', '').replace(']', '').replace('\"', '')\n",
    "    \n",
    "    ingredients_str = ''.join([f\", {ingr}\" for ingr in ingredients])\n",
    "    ingredients_str = ingredients_str.replace(\", \", \"\", 1)\n",
    "    \n",
    "    techniques_str = ''.join([f\", {tec}\" for tec in techniques])\n",
    "    techniques_str = techniques_str.replace(\", \", \"\", 1)\n",
    "\n",
    "    recipe_instance = f\"[BOS] [INGREDIENTS] {ingredients_str} [TECHNIQUES] {techniques_str} [STEPS] {instructions} [EOS]\"\n",
    "    return recipe_instance"
   ],
   "id": "1e29c3209be883b2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:46.504106Z",
     "start_time": "2024-10-08T09:31:46.501590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# Funzione per preprocessare i dati in parallelo\n",
    "def load_preprocess_raw_data_parallel(raw_data):\n",
    "    with open(raw_data, 'r', encoding='utf-8') as f:\n",
    "        reader = list(csv.DictReader(f))  # Convertiamo reader in una lista\n",
    "        num_cores = multiprocessing.cpu_count()  # Otteniamo il numero di core della CPU\n",
    "        print(f\"Number of CPU cores: {num_cores}\")\n",
    "        \n",
    "        # Eseguiamo il preprocessing in parallelo\n",
    "        recipe_instances = Parallel(n_jobs=num_cores)(\n",
    "            delayed(process_recipe)(row) for row in tqdm(reader, desc=\"Processing recipes\", unit=\"recipes\")\n",
    "        )\n",
    "\n",
    "    return recipe_instances"
   ],
   "id": "5e8da04f0289a7e3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:51.984568Z",
     "start_time": "2024-10-08T09:31:46.510782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create text list for dataset\n",
    "# https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions/data\n",
    "#recipe_list = load_preprocess_raw_data(\"dataset/RAW_merged.csv\")\n",
    "recipe_list = load_preprocess_raw_data_parallel(\"dataset/RAW_merged_top_smallest.csv\")\n",
    "recipe_list = recipe_list[:100]\n",
    "\n",
    "train_list, test_list = np.split(recipe_list, [int(.8 * len(recipe_list))])\n",
    "print('\\nNumber of train data: ', len(train_list))\n",
    "print('Number of test data: ', len(test_list))"
   ],
   "id": "7305fcc41d562385",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing recipes: 100%|██████████| 6010/6010 [00:04<00:00, 1227.87recipes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of train data:  80\n",
      "Number of test data:  20\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:52.048785Z",
     "start_time": "2024-10-08T09:31:52.046681Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_list[0])",
   "id": "c2ec262f1e1647a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BOS] [INGREDIENTS] virgin oil, rom tomato, clove, juice, basil, salt [TECHNIQUES] boil, grill, marinate, simmer, toss [STEPS] mix all ingredients together and set on counter stirring occasionally, the salt will bring out the juice of the tomatoes, taste and add more seasonings to your personal taste, you can use fresh basil or oregano if preferred , just remember to chopped finely and use a little less than dried, uses: serve with almost any meal as a side, on top of a green salad, use as a topping for bruschetta--just chop tomatoes instead of slicing, use for caprese salad--add sliced buffalo mozzarella and youre done, have some leftover juice: place some sliced chicken breasts and marinate over night , then carefully remove the chicken and quickly saut them until done, add remaining juice and tomatoes , boil for 1-2 minutes and then add cooked penne pasta , toss , and simmer for 5-6 minutes, juice uses: you can reuse the marinade with new tomatoes, just add some more sliced tomatoes to the juice and let sit for about 20 minutes, or use the juice and marinate boneless chicken breast overnight in refrigerator , grill and serve or slice chicken and top a caesar salad with it [EOS]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:52.709942Z",
     "start_time": "2024-10-08T09:31:52.136155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the GPT tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, bos_token='[BOS]', eos_token='[EOS]', pad_token='[PAD]')\n",
    "# add special tokens for title, ingredients and instruction seperator\n",
    "special_tokens_dict = {'additional_special_tokens': ['[INGREDIENTS]', '[TECHNIQUES]', '[STEPS]']} # '[INGR]', '[STEP]'\n",
    "# check the number of special tokens\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print('We have added', num_added_toks, 'tokens')"
   ],
   "id": "94f53b4d8b82cb3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 3 tokens\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:52.720494Z",
     "start_time": "2024-10-08T09:31:52.718475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verifica gli ID dei token speciali\n",
    "special_tokens = ['[BOS]', '[EOS]', '[PAD]', '[INGREDIENTS]', '[TECHNIQUES]', '[STEPS]'] # '[INGR]'\n",
    "for token in special_tokens:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"Token: {token}, ID: {token_id}\")"
   ],
   "id": "bf990733c71b6569",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [BOS], ID: 50257\n",
      "Token: [EOS], ID: 50258\n",
      "Token: [PAD], ID: 50259\n",
      "Token: [INGREDIENTS], ID: 50260\n",
      "Token: [TECHNIQUES], ID: 50261\n",
      "Token: [STEPS], ID: 50262\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:52.745848Z",
     "start_time": "2024-10-08T09:31:52.733942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Esempio di testi\n",
    "texts = [\n",
    "    train_list[0]\n",
    "]\n",
    "\n",
    "# Tokenizzazione\n",
    "encodings = tokenizer.batch_encode_plus(\n",
    "    texts,\n",
    "    truncation=True,\n",
    "    max_length=320,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt',\n",
    "    add_special_tokens=True  # Assicurati che i token speciali siano inclusi\n",
    ")\n",
    "\n",
    "# Visualizza gli input_ids e attention_mask\n",
    "print(\"Input IDs:\")\n",
    "print(encodings['input_ids'])\n",
    "print(\"\\nAttention Mask:\")\n",
    "print(encodings['attention_mask'])"
   ],
   "id": "c64c7c9f25db5f5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:\n",
      "tensor([[50257,   220, 50260, 21772,  3056,    11,  9267, 24240,    11,   537,\n",
      "           659,    11, 13135,    11, 37792,    11,  8268,   220, 50261, 20667,\n",
      "            11, 29901,    11,  1667,  4559,    11, 32857,    11, 12153,   220,\n",
      "         50262,  5022,   477,  9391,  1978,   290,   900,   319,  3753, 26547,\n",
      "         10491,    11,   262,  8268,   481,  2222,   503,   262, 13135,   286,\n",
      "           262, 23972,    11,  6938,   290,   751,   517,  1622,   654,   284,\n",
      "           534,  2614,  6938,    11,   345,   460,   779,  4713, 37792,   393,\n",
      "         23751,  1030,    78,   611,  9871,   837,   655,  3505,   284, 20720,\n",
      "         32566,   290,   779,   257,  1310,  1342,   621, 16577,    11,  3544,\n",
      "            25,  4691,   351,  2048,   597,  9799,   355,   257,  1735,    11,\n",
      "           319,  1353,   286,   257,  4077, 20698,    11,   779,   355,   257,\n",
      "         34366,   329,   865,   385,  2395, 25854,   438,  3137, 30506, 23972,\n",
      "          2427,   286, 49289,    11,   779,   329,  1451,   260,   325, 20698,\n",
      "           438,  2860, 26790, 45019,  6941,  3019, 45494,   290,   345,   260,\n",
      "          1760,    11,   423,   617, 39191, 13135,    25,  1295,   617, 26790,\n",
      "          9015, 17515,   290,  1667,  4559,   625,  1755,   837,   788,  7773,\n",
      "          4781,   262,  9015,   290,  2952,   473,   315,   606,  1566,  1760,\n",
      "            11,   751,  5637, 13135,   290, 23972,   837, 20667,   329,   352,\n",
      "            12,    17,  2431,   290,   788,   751, 15847,  3112,   710, 26296,\n",
      "           837, 12153,   837,   290, 32857,   329,   642,    12,    21,  2431,\n",
      "            11, 13135,  3544,    25,   345,   460, 32349,   262,  1667,   259,\n",
      "           671,   351,   649, 23972,    11,   655,   751,   617,   517, 26790,\n",
      "         23972,   284,   262, 13135,   290,  1309,  1650,   329,   546,  1160,\n",
      "          2431,    11,   393,   779,   262, 13135,   290,  1667,  4559,  5351,\n",
      "          5321,  9015,  9296, 13417,   287, 30500,   837, 29901,   290,  4691,\n",
      "           393, 16416,  9015,   290,  1353,   257,  1275, 18964, 20698,   351,\n",
      "           340,   220, 50258, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259]])\n",
      "\n",
      "Attention Mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:52.758534Z",
     "start_time": "2024-10-08T09:31:52.756077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Decodifica dei token\n",
    "decoded_inputs = [tokenizer.decode(ids, skip_special_tokens=False) for ids in encodings['input_ids']]\n",
    "for i, decoded in enumerate(decoded_inputs):\n",
    "    print(f\"Decoded Input {i}: {decoded}\")"
   ],
   "id": "7c18a81d35858df6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Input 0: [BOS]  [INGREDIENTS]  virgin oil, rom tomato, clove, juice, basil, salt  [TECHNIQUES]  boil, grill, marinate, simmer, toss  [STEPS]  mix all ingredients together and set on counter stirring occasionally, the salt will bring out the juice of the tomatoes, taste and add more seasonings to your personal taste, you can use fresh basil or oregano if preferred, just remember to chopped finely and use a little less than dried, uses: serve with almost any meal as a side, on top of a green salad, use as a topping for bruschetta--just chop tomatoes instead of slicing, use for caprese salad--add sliced buffalo mozzarella and youre done, have some leftover juice: place some sliced chicken breasts and marinate over night, then carefully remove the chicken and quickly saut them until done, add remaining juice and tomatoes, boil for 1-2 minutes and then add cooked penne pasta, toss, and simmer for 5-6 minutes, juice uses: you can reuse the marinade with new tomatoes, just add some more sliced tomatoes to the juice and let sit for about 20 minutes, or use the juice and marinate boneless chicken breast overnight in refrigerator, grill and serve or slice chicken and top a caesar salad with it [EOS][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:52.831848Z",
     "start_time": "2024-10-08T09:31:52.774266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lengths = [len(tokenizer.encode(recipe)) for recipe in recipe_list]\n",
    "max_length_in_data = max(lengths)\n",
    "avg_length_in_data = sum(lengths) / len(lengths)\n",
    "print(f\"Lunghezza massima: {max_length_in_data}, Lunghezza media: {avg_length_in_data}\")\n",
    "\n",
    "# Lunghezza massima: 312, Lunghezza media: 136.59567387687187"
   ],
   "id": "28e471a9fc239fee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lunghezza massima: 293, Lunghezza media: 137.77\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:52.845418Z",
     "start_time": "2024-10-08T09:31:52.841636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length=320):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.texts = [text.lower() for text in txt_list]  # Preprocess texts to lowercase\n",
    "        \n",
    "        # Tokenize all texts in a batch\n",
    "        encodings = tokenizer.batch_encode_plus(\n",
    "            self.texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        self.input_ids = encodings['input_ids']\n",
    "        self.attn_masks = encodings['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ],
   "id": "78bf90f4f0831e0a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:52.894149Z",
     "start_time": "2024-10-08T09:31:52.854770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = GPT2Dataset(train_list, tokenizer, max_length=320)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ],
   "id": "7161f97045a959b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   64 training samples\n",
      "   16 validation samples\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:52.906794Z",
     "start_time": "2024-10-08T09:31:52.904147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,  # The training samples.\n",
    "    sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
    "    batch_size=batch_size  # Trains with this batch size.\n",
    ")\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,  # The validation samples.\n",
    "    sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
    "    batch_size=batch_size  # Evaluate with this batch size.\n",
    ")"
   ],
   "id": "33c8951c033ef8d4",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:55.536681Z",
     "start_time": "2024-10-08T09:31:52.917602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained(model_name, output_hidden_states=False)\n",
    "\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed_val)"
   ],
   "id": "3dcdc139a16bf4c1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:31:55.563501Z",
     "start_time": "2024-10-08T09:31:55.559401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=learning_rate,\n",
    "                              eps=epsilon\n",
    "                              )\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "print('Total number of steps: ', total_steps)\n",
    "\n",
    "warmup_steps = int(0.05 * total_steps) # 5% del totale degli step\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)"
   ],
   "id": "92ba0b6b2835fe63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps:  16\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:32:48.539768Z",
     "start_time": "2024-10-08T09:31:55.584121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_stats = []\n",
    "print(\"Currently using device type: \", device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    for step, batch in enumerate(loop):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids,\n",
    "                        labels=b_labels,\n",
    "                        attention_mask=b_masks\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        losses.append(batch_loss)\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "            print('Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.'.format(step, len(train_dataloader), batch_loss))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % save_every == 0:\n",
    "            model.save_pretrained(save_model)\n",
    "\n",
    "        loop.set_postfix(loss=batch_loss)\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Calculate perplexity.\n",
    "    losses = torch.tensor(losses)\n",
    "    train_perplexity = math.exp(torch.mean(losses))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Perplexity: {0:.2f}\".format(train_perplexity))\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids,\n",
    "                            attention_mask=b_masks,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        losses.append(batch_loss)\n",
    "        total_eval_loss += batch_loss\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    # Calculate perplexity.\n",
    "    losses = torch.tensor(losses)\n",
    "    val_perplexity = math.exp(torch.mean(losses))\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation perplexity: {0:.2f}\".format(val_perplexity))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Perplexity': train_perplexity,\n",
    "            'Valid. Perplexity': val_perplexity,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ],
   "id": "c9378deeb812b50f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using device type:  cpu\n",
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:49<00:00,  3.11s/it, loss=3.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 11.65\n",
      "  Perplexity: 115188.20\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 4.49\n",
      "  Validation perplexity: 89.30\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:32:48.547667Z",
     "start_time": "2024-10-08T09:32:48.545424Z"
    }
   },
   "cell_type": "code",
   "source": "#model.save_pretrained(save_model)",
   "id": "b6b677c0bd507384",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:32:48.692860Z",
     "start_time": "2024-10-08T09:32:48.690847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # prepare datasets for dev_list and test_list\n",
    "# test_dataset = GPT2Dataset(test_list, tokenizer, max_length=320)\n",
    "# # load the datasets\n",
    "# test_dataloader = DataLoader(\n",
    "#     test_dataset,  # The validation samples.\n",
    "#     sampler=SequentialSampler(test_dataset),  # Pull out batches sequentially.\n",
    "#     batch_size=batch_size  # Evaluate with this batch size.\n",
    "# )"
   ],
   "id": "ac66cf7cbc079922",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:32:48.825916Z",
     "start_time": "2024-10-08T09:32:48.823863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def evaluate_model(model, dataloaded):\n",
    "#     model = model.to(device)\n",
    "#     model.eval()\n",
    "# \n",
    "#     losses = []\n",
    "#     perplexity = []\n",
    "#     total_eval_loss = 0\n",
    "# \n",
    "#     # Evaluate data for one epoch\n",
    "#     for batch in dataloaded:\n",
    "#         b_input_ids = batch[0].to(device)\n",
    "#         b_labels = batch[0].to(device)\n",
    "#         b_masks = batch[1].to(device)\n",
    "# \n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(b_input_ids,\n",
    "#                             attention_mask=b_masks,\n",
    "#                             labels=b_labels)\n",
    "# \n",
    "#             loss = outputs[0]\n",
    "# \n",
    "#         batch_loss = loss.item()\n",
    "#         losses.append(batch_loss)\n",
    "#         total_eval_loss += batch_loss\n",
    "# \n",
    "#     avg_val_loss = total_eval_loss / len(dataloaded)\n",
    "# \n",
    "#     # Calculate perplexity.\n",
    "#     losses = torch.tensor(losses)\n",
    "#     val_perplexity = math.exp(torch.mean(losses))\n",
    "#     perplexity.append(val_perplexity)\n",
    "# \n",
    "#     print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "#     print(\"  Validation perplexity: {0:.2f}\".format(val_perplexity))\n",
    "#     return avg_val_loss, val_perplexity"
   ],
   "id": "3bbda5e5a4f70139",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:32:48.955754Z",
     "start_time": "2024-10-08T09:32:48.953929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print('Testing...')\n",
    "# test_loss, test_perplexity = evaluate_model(model, test_dataloader)\n",
    "# test_eval_df = pd.DataFrame(columns=[\"test_loss\", \"test_perplexity\"])\n",
    "# test_eval_df['test_loss'] = test_loss\n",
    "# test_eval_df['test_perplexity'] = test_perplexity\n",
    "# test_eval_df.to_csv(\"test_eval.csv\")"
   ],
   "id": "148a48edac7e6675",
   "outputs": [],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
