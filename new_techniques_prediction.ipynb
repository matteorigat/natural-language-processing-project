{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:07:30.225069Z",
     "start_time": "2025-04-26T11:07:29.971899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize, pos_tag\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Importa tqdm per la barra di progresso\n",
    "\n",
    "# Scarica i dati necessari di NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "id": "3dd62951cdc8aabf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matteorigat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matteorigat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/matteorigat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/matteorigat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:07:30.234476Z",
     "start_time": "2025-04-26T11:07:30.228258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inizializza le stopwords di NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Funzione di pulizia e preprocessing avanzato dei dati per gli ingredienti\n",
    "def preprocess_ingredients(ingredients):\n",
    "    ingredients_list = eval(ingredients)\n",
    "    processed_ingredients = []\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # POS tags that represent nouns\n",
    "    noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "    # Define the words to be dropped\n",
    "    #words_to_drop = {\"powder\", \"brown\", \"salt\", \"water\", \"sugar\", \"onion\", \"butter\", \"pepper\", \"ground\", \"cream\"} \n",
    "\n",
    "    for ingr in ingredients_list:\n",
    "        ingr = regex.sub(' ', ingr.lower()).strip()\n",
    "        components = [comp.strip() for comp in ingr.split('and')]\n",
    "        \n",
    "        for comp in components:\n",
    "            sentence = \"\"\n",
    "            tokens = word_tokenize(comp)  # Tokenize each component\n",
    "            tagged_tokens = pos_tag(tokens)  # Perform POS tagging\n",
    "            \n",
    "            # Extract main nouns while handling compound nouns\n",
    "            nouns = []\n",
    "            current_noun = \"\"\n",
    "            for word, tag in tagged_tokens:\n",
    "                word = lemmatizer.lemmatize(word.strip())\n",
    "                if len(word) > 2 and word not in stop_words and tag in noun_tags: #and word not in words_to_drop\n",
    "                    if current_noun:\n",
    "                        nouns.append(current_noun)\n",
    "                        current_noun = \"\"\n",
    "                    current_noun = word\n",
    "            \n",
    "            # Add last current noun if exists\n",
    "            if current_noun:\n",
    "                nouns.append(current_noun)            \n",
    "            \n",
    "            for word in nouns:\n",
    "                singular_comp = lemmatizer.lemmatize(word.strip())\n",
    "                if singular_comp not in stop_words and len(singular_comp) > 2:\n",
    "                    sentence += singular_comp + \" \"\n",
    "                    \n",
    "            if sentence.strip():\n",
    "                processed_ingredients.append(sentence.strip())\n",
    "\n",
    "    return list(set(processed_ingredients))\n",
    "\n",
    "# Funzione di preprocessing per le tecniche\n",
    "def preprocess_techniques(techniques):\n",
    "    techniques_list = eval(techniques)\n",
    "    processed_techniques = []\n",
    "\n",
    "    for technique in techniques_list:\n",
    "        technique = technique.lower().strip()\n",
    "        tokens = word_tokenize(technique)\n",
    "        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "        processed_techniques.append(' '.join(tokens))\n",
    "\n",
    "    return processed_techniques"
   ],
   "id": "6970b10be13c7cc4",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:10:02.934472Z",
     "start_time": "2025-04-26T11:07:30.270932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Carica il dataset\n",
    "df = pd.read_csv('dataset/RAW_merged.csv')\n",
    "\n",
    "tqdm.pandas(desc=\"Processing Ingredients\")\n",
    "df['ingredients_processed'] = df['ingredients'].progress_apply(preprocess_ingredients)\n",
    "# Applica il preprocessing sugli ingredienti\n",
    "#df['ingredients_processed_spacy'] = df['ingredients'].progress_apply(preprocess_ingredients_spacy)\n",
    "\n",
    "tqdm.pandas(desc=\"Processing Techniques\")\n",
    "df['techniques_processed'] = df['techniques_list'].progress_apply(preprocess_techniques)\n",
    "\n",
    "print(df[['ingredients', 'ingredients_processed', 'techniques_list', 'techniques_processed']].head())"
   ],
   "id": "578e7da9f9d9176b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Ingredients: 100%|██████████| 178265/178265 [02:22<00:00, 1247.34it/s]\n",
      "Processing Techniques: 100%|██████████| 178265/178265 [00:08<00:00, 20537.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         ingredients  \\\n",
      "0  ['winter squash', 'mexican seasoning', 'mixed ...   \n",
      "1  ['prepared pizza crust', 'sausage patty', 'egg...   \n",
      "2  ['spreadable cheese with garlic and herbs', 'n...   \n",
      "3  ['milk', 'vanilla ice cream', 'frozen apple ju...   \n",
      "4  ['fennel seeds', 'green olives', 'ripe olives'...   \n",
      "\n",
      "                               ingredients_processed  \\\n",
      "0  [seasoning, salt, honey, spice, oil, winter sq...   \n",
      "1  [egg, milk, salt, sausage patty, pizza crust, ...   \n",
      "2  [potato, salt, shallot, wine vinegar, tarragon...   \n",
      "3  [milk, apple, apple juice concentrate, vanilla...   \n",
      "4  [fennel seed, garlic, orange juice, peppercorn...   \n",
      "\n",
      "                                 techniques_list  \\\n",
      "0                      ['bake', 'grate', 'melt']   \n",
      "1                      ['bake', 'pour', 'whisk']   \n",
      "2              ['bake', 'boil', 'dice', 'drain']   \n",
      "3                 ['blend', 'combine', 'smooth']   \n",
      "4  ['crush', 'marinate', 'refrigerate', 'toast']   \n",
      "\n",
      "                    techniques_processed  \n",
      "0                    [bake, grate, melt]  \n",
      "1                    [bake, pour, whisk]  \n",
      "2              [bake, boil, dice, drain]  \n",
      "3               [blend, combine, smooth]  \n",
      "4  [crush, marinate, refrigerate, toast]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:10:18.746754Z",
     "start_time": "2025-04-26T11:10:02.945890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# df['ingredients'] = df['ingredients'].apply(eval)\n",
    "# df['techniques'] = df['techniques_list'].apply(lambda x: eval(x) if pd.notnull(x) else [])\n",
    "\n",
    "all_texts = df['ingredients_processed'] + df['techniques_processed']\n",
    "\n",
    "model = Word2Vec(all_texts, vector_size=100, window=5, min_count=1, sg=1, epochs=10)\n",
    "\n",
    "ingredients_vectors = {ingredient: model.wv[ingredient] for ingredient in model.wv.index_to_key}\n",
    "techniques_vectors = {technique: model.wv[technique] for technique in df['techniques_processed'].explode().dropna().unique() if technique in model.wv}"
   ],
   "id": "6b5c67de25eb71c3",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T14:42:37.821102Z",
     "start_time": "2025-04-26T14:42:37.778789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import os # Import the os module\n",
    "\n",
    "# ... (your existing code up to the creation of techniques_vectors)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "model_dir = \"models\"\n",
    "os.makedirs(model_dir, exist_ok=True) # exist_ok=True prevents error if directory already exists\n",
    "\n",
    "# Save the Word2Vec model (as you already do)\n",
    "model.save(os.path.join(model_dir, \"word2vec_ingredients_techniques2.model\"))\n",
    "print(f\"Word2Vec model saved to {os.path.join(model_dir, 'word2vec_ingredients_techniques2.model')}\")\n",
    "\n",
    "# Save the techniques_vectors dictionary using pickle\n",
    "techniques_vectors_path = os.path.join(model_dir, \"techniques_vectors2.pkl\")\n",
    "with open(techniques_vectors_path, 'wb') as f:\n",
    "    pickle.dump(techniques_vectors, f)\n",
    "print(f\"Techniques vectors dictionary saved to {techniques_vectors_path}\")\n",
    "\n",
    "# ... (rest of your script, like the prediction example)"
   ],
   "id": "73b6604bd84a7ad5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved to models/word2vec_ingredients_techniques2.model\n",
      "Techniques vectors dictionary saved to models/techniques_vectors.pkl\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:10:18.781267Z",
     "start_time": "2025-04-26T11:10:18.750549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_ingredients(ingredients):\n",
    "    processed_ingredients = []\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # POS tags that represent nouns\n",
    "    noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "    # Define the words to be dropped\n",
    "    #words_to_drop = {\"powder\", \"brown\", \"salt\", \"water\", \"sugar\", \"onion\", \"butter\", \"pepper\", \"ground\", \"cream\"} \n",
    "\n",
    "    for ingr in ingredients:\n",
    "        ingr = regex.sub(' ', ingr.lower()).strip()\n",
    "        components = [comp.strip() for comp in ingr.split('and')]\n",
    "        \n",
    "        for comp in components:\n",
    "            sentence = \"\"\n",
    "            tokens = word_tokenize(comp)  # Tokenize each component\n",
    "            tagged_tokens = pos_tag(tokens)  # Perform POS tagging\n",
    "            \n",
    "            # Extract main nouns while handling compound nouns\n",
    "            nouns = []\n",
    "            current_noun = \"\"\n",
    "            for word, tag in tagged_tokens:\n",
    "                word = lemmatizer.lemmatize(word.strip())\n",
    "                if len(word) > 2 and word not in stop_words and tag in noun_tags: #and word not in words_to_drop\n",
    "                    if current_noun:\n",
    "                        nouns.append(current_noun)\n",
    "                        current_noun = \"\"\n",
    "                    current_noun = word\n",
    "            \n",
    "            # Add last current noun if exists\n",
    "            if current_noun:\n",
    "                nouns.append(current_noun)            \n",
    "            \n",
    "            for word in nouns:\n",
    "                singular_comp = lemmatizer.lemmatize(word.strip())\n",
    "                if singular_comp not in stop_words and len(singular_comp) > 2:\n",
    "                    sentence += singular_comp + \" \"\n",
    "                    \n",
    "            if sentence.strip():\n",
    "                processed_ingredients.append(sentence.strip())\n",
    "\n",
    "    return list(set(processed_ingredients))"
   ],
   "id": "20527b549f1983f1",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:10:18.793459Z",
     "start_time": "2025-04-26T11:10:18.789864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def predict_cooking_methods(ingredients, techniques_vectors, model, top_n=3):\n",
    "    ingredients = [ingredient.strip() for ingredient in ingredients.split(\",\")]\n",
    "    print(ingredients)\n",
    "    ingredient_list = preprocess_ingredients(ingredients)\n",
    "    print(ingredient_list)\n",
    "    ingredient_vectors = [model.wv[ingredient] for ingredient in ingredient_list if ingredient in model.wv]\n",
    "    \n",
    "    if len(ingredient_vectors) == 0:\n",
    "        return \"Ingredienti non trovati nel vocabolario.\"\n",
    "\n",
    "    avg_ingredient_vector = np.mean(ingredient_vectors, axis=0).reshape(1, -1)\n",
    "\n",
    "    similarities = {}\n",
    "    for technique, technique_vector in techniques_vectors.items():\n",
    "        normalized_technique_vector = normalize(np.array(technique_vector).reshape(1, -1))\n",
    "        similarities[technique] = cosine_similarity(avg_ingredient_vector, normalized_technique_vector)[0][0]\n",
    "\n",
    "    sorted_techniques = sorted(similarities, key=similarities.get, reverse=True)\n",
    "    \n",
    "    return sorted_techniques[:top_n] if len(sorted_techniques) > 0 else \"Nessuna tecnica di cottura trovata.\""
   ],
   "id": "f90992061a8fde",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:11:38.621850Z",
     "start_time": "2025-04-26T11:11:38.597956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Lista di ingredienti di esempio\n",
    "new_ingredients = \"pasta, onion, tomato, olive oil, salt\"\n",
    "\n",
    "# Predizione\n",
    "predicted_methods = predict_cooking_methods(new_ingredients, techniques_vectors, model)\n",
    "print(\"Metodi di cottura predetti:\", predicted_methods)"
   ],
   "id": "173857e739491fec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pasta', 'onion', 'tomato', 'olive oil', 'salt']\n",
      "['tomato', 'pasta', 'salt', 'onion', 'oil']\n",
      "Metodi di cottura predetti: ['parboil', 'dice', 'drain']\n"
     ]
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
